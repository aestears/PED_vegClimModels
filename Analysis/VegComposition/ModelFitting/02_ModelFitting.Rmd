---
title: "Models of vegetation composition based on climate predictors"
author: "Alice Stears"
date: "`r lubridate::today()`"
output:
  html_document:
    toc: true
    df_print: paged
    code_folding: hide
params:
  run: false
  test_run: false
  save_figs: false
  ecoregion: shrubGrass
  response: TotalHerbaceousCover
  hmod: false
  s: TotalHerbaceousCover
  sample_group: 1
  byRegion: true
subtitle: "Based on code from Martin Holdrege's Sagebrush-Fire paper"
---
The data consists of vegetation % cover by functional group from across CONUS (from AIM, FIA, LANDFIRE, and RAP), 
as well as climate variables from DayMet, which have been aggregated into mean interannual conditions accross multiple temporal windows. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE,
                      message = FALSE)
```

# Dependencies 

User defined parameters

```{r}
print(params)
# set to true if want to run for a limited number of rows (i.e. for code testing)
test_run <- params$test_run
save_figs <- params$save_figs
hmod <- params$hmod # whether to include human modification in the model
# by changing the sample_group the model can be fit to a completely different set of rows
sample_group <- params$sample_group 
response <- params$response
# _ann defines this model as being built on annual data
s <- params$s # string to paste to file names e.g., that defines the interactions in the model
# such as (summer ppt * MAT) and annuals*temperature interactions
fit_sample <- TRUE # fit model to a sample of the data
n_train <- 5e4 # sample size of the training data
n_test <- 1e6 # sample size of the testing data (if this is too big the decile dotplot code throws memory errors)
byRegion <- params$byRegion

run <- params$run
```


```{r warning=FALSE, message=FALSE}
# set option so resampled dataset created here reproduces earlier runs of this code with dplyr 1.0.10
source("../../../Functions/glmTransformsIterates.R")
source("../../../Functions/transformPreds.R")
source("../../../Functions/StepBeta_mine.R")
#source("src/fig_params.R")
#source("src/modeling_functions.R")
 
library(ggspatial)
library(terra)
library(tidyterra)
library(sf)
library(caret)
library(tidyverse)
library(GGally) # for ggpairs()
library(pdp) # for partial dependence plots
library(gridExtra)
library(knitr)
library(patchwork) # for figure insets etc. 
library(ggtext)
library(StepBeta)
theme_set(theme_classic())
library(here)
library(rsample)
library(kableExtra)
library(glmnet)
```

# read in data

Data compiled in the `prepDataForModels.R` script

```{r}
here::i_am("Analysis/VegComposition/ModelFitting/02_ModelFitting.Rmd")
modDat <- readRDS( here("Data_processed", "CoverData", "DataForModels_spatiallyAveraged_withSoils_noSf.rds"))
## there are some values of the annual wet degree days 5th percentile that have -Inf?? change to lowest value for now? 
modDat[is.infinite(modDat$annWetDegDays_5percentile_3yrAnom), "annWetDegDays_5percentile_3yrAnom"] <- -47.8
## same, but for annual water deficit 95th percentile 
modDat[is.infinite(modDat$annWaterDeficit_95percentile_3yrAnom), "annWaterDeficit_95percentile_3yrAnom"] <- -600

# # Convert total cover variables into proportions (for later use in beta regression models) ... proportions are already scaled from zero to 1
# modDat <- modDat %>%
#   mutate(TotalTreeCover = TotalTreeCover/100,
#          CAMCover = CAMCover/100,
#          TotalHerbaceousCover = TotalHerbaceousCover/100,
#          BareGroundCover = BareGroundCover/100,
#          ShrubCover = ShrubCover/100
#          )
# For all response variables, make sure there are no 0s add or subtract .0001 from each, since the Gamma model framework can't handle that
modDat[modDat$TotalTreeCover == 0 & !is.na(modDat$TotalTreeCover), "TotalTreeCover"] <- 0.0001
modDat[modDat$CAMCover == 0 & !is.na(modDat$CAMCover), "CAMCover"] <- 0.0001
modDat[modDat$TotalHerbaceousCover == 0  & !is.na(modDat$TotalHerbaceousCover), "TotalHerbaceousCover"] <- 0.0001
modDat[modDat$BareGroundCover == 0 & !is.na(modDat$BareGroundCover), "BareGroundCover"] <- 0.0001
modDat[modDat$ShrubCover == 0 & !is.na(modDat$ShrubCover), "ShrubCover"] <- 0.0001
modDat[modDat$BroadleavedTreeCover_prop == 0 & !is.na(modDat$BroadleavedTreeCover_prop), "BroadleavedTreeCover_prop"] <- 0.0001
modDat[modDat$NeedleLeavedTreeCover_prop == 0 & !is.na(modDat$NeedleLeavedTreeCover_prop), "NeedleLeavedTreeCover_prop"] <- 0.0001
modDat[modDat$C4Cover_prop == 0 & !is.na(modDat$C4Cover_prop), "C4Cover_prop"] <- 0.0001
modDat[modDat$C3Cover_prop == 0 & !is.na(modDat$C3Cover_prop), "C3Cover_prop"] <- 0.0001
modDat[modDat$ForbCover_prop == 0 & !is.na(modDat$ForbCover_prop), "ForbCover_prop"] <- 0.0001
# 
# modDat[modDat$TotalTreeCover ==1& !is.na(modDat$TotalTreeCover), "TotalTreeCover"] <- 0.999
# modDat[modDat$CAMCover ==1& !is.na(modDat$CAMCover), "CAMCover"] <- 0.999
# modDat[modDat$TotalHerbaceousCover ==1 & !is.na(modDat$TotalHerbaceousCover), "TotalHerbaceousCover"] <- 0.999
# modDat[modDat$BareGroundCover ==1& !is.na(modDat$BareGroundCover), "BareGroundCover"] <- 0.999
# modDat[modDat$ShrubCover ==1& !is.na(modDat$ShrubCover), "ShrubCover"] <- 0.999
# modDat[modDat$BroadleavedTreeCover_prop ==1& !is.na(modDat$BroadleavedTreeCover_prop), "BroadleavedTreeCover_prop"] <- 0.999
# modDat[modDat$NeedleLeavedTreeCover_prop ==1& !is.na(modDat$NeedleLeavedTreeCover_prop), "NeedleLeavedTreeCover_prop"] <- 0.999
# modDat[modDat$C4Cover_prop ==1& !is.na(modDat$C4Cover_prop), "C4Cover_prop"] <- 0.999
# modDat[modDat$C3Cover_prop ==1& !is.na(modDat$C3Cover_prop), "C3Cover_prop"] <- 0.999
# modDat[modDat$ForbCover_prop ==1& !is.na(modDat$ForbCover_prop), "ForbCover_prop"] <- 0.999

```


# Prep data

```{r rename variables}
set.seed(1234)
modDat_1 <- modDat %>% 
  select(-c(prcp_annTotal:annVPD_min)) %>% 
  # mutate(Lon = st_coordinates(.)[,1], 
  #        Lat = st_coordinates(.)[,2])  %>% 
  # st_drop_geometry() %>% 
  # filter(!is.na(newRegion))
  rename("tmin" = tmin_meanAnnAvg_CLIM, 
     "tmax" = tmax_meanAnnAvg_CLIM, #1
     "tmean" = tmean_meanAnnAvg_CLIM, 
     "prcp" = prcp_meanAnnTotal_CLIM, 
     "t_warm" = T_warmestMonth_meanAnnAvg_CLIM,
     "t_cold" = T_coldestMonth_meanAnnAvg_CLIM, 
     "prcp_wet" = precip_wettestMonth_meanAnnAvg_CLIM,
     "prcp_dry" = precip_driestMonth_meanAnnAvg_CLIM, 
     "prcp_seasonality" = precip_Seasonality_meanAnnAvg_CLIM, #2
     "prcpTempCorr" = PrecipTempCorr_meanAnnAvg_CLIM,  #3
     "abvFreezingMonth" = aboveFreezing_month_meanAnnAvg_CLIM, 
     "isothermality" = isothermality_meanAnnAvg_CLIM, #4
     "annWatDef" = annWaterDeficit_meanAnnAvg_CLIM, 
     "annWetDegDays" = annWetDegDays_meanAnnAvg_CLIM,
     "VPD_mean" = annVPD_mean_meanAnnAvg_CLIM, 
     "VPD_max" = annVPD_max_meanAnnAvg_CLIM, #5
     "VPD_min" = annVPD_min_meanAnnAvg_CLIM, #6
     "VPD_max_95" = annVPD_max_95percentile_CLIM, 
     "annWatDef_95" = annWaterDeficit_95percentile_CLIM, 
     "annWetDegDays_5" = annWetDegDays_5percentile_CLIM, 
     "frostFreeDays_5" = durationFrostFreeDays_5percentile_CLIM, 
     "frostFreeDays" = durationFrostFreeDays_meanAnnAvg_CLIM, 
     "soilDepth" = soilDepth, #7
     "clay" = surfaceClay_perc, 
     "sand" = avgSandPerc_acrossDepth, #8
     "coarse" = avgCoarsePerc_acrossDepth, #9
     "carbon" = avgOrganicCarbonPerc_0_3cm, #10
     "AWHC" = totalAvailableWaterHoldingCapacity,
     ## anomaly variables
     tmean_anom = tmean_meanAnnAvg_3yrAnom, #15
     tmin_anom = tmin_meanAnnAvg_3yrAnom, #16
     tmax_anom = tmax_meanAnnAvg_3yrAnom, #17
    prcp_anom = prcp_meanAnnTotal_3yrAnom, #18
      t_warm_anom = T_warmestMonth_meanAnnAvg_3yrAnom,  #19
     t_cold_anom = T_coldestMonth_meanAnnAvg_3yrAnom, #20
      prcp_wet_anom = precip_wettestMonth_meanAnnAvg_3yrAnom, #21
      precp_dry_anom = precip_driestMonth_meanAnnAvg_3yrAnom,  #22
    prcp_seasonality_anom = precip_Seasonality_meanAnnAvg_3yrAnom, #23 
     prcpTempCorr_anom = PrecipTempCorr_meanAnnAvg_3yrAnom, #24
      aboveFreezingMonth_anom = aboveFreezing_month_meanAnnAvg_3yrAnom, #25  
    isothermality_anom = isothermality_meanAnnAvg_3yrAnom, #26
       annWatDef_anom = annWaterDeficit_meanAnnAvg_3yrAnom, #27
     annWetDegDays_anom = annWetDegDays_meanAnnAvg_3yrAnom,  #28
      VPD_mean_anom = annVPD_mean_meanAnnAvg_3yrAnom, #29
      VPD_min_anom = annVPD_min_meanAnnAvg_3yrAnom,  #30
      VPD_max_anom = annVPD_max_meanAnnAvg_3yrAnom,  #31
     VPD_max_95_anom = annVPD_max_95percentile_3yrAnom, #32
      annWatDef_95_anom = annWaterDeficit_95percentile_3yrAnom, #33 
      annWetDegDays_5_anom = annWetDegDays_5percentile_3yrAnom ,  #34
    frostFreeDays_5_anom = durationFrostFreeDays_5percentile_3yrAnom, #35 
      frostFreeDays_anom = durationFrostFreeDays_meanAnnAvg_3yrAnom #36
  )

# small dataset for if testing the data
if(test_run) {
  modDat_1 <- slice_sample(modDat_1, n = 1e5)
}
```

Identify the ecoregion and response variable type to use in this model run 
```{r get ecoregion variable}
ecoregion <- params$ecoregion
response <- params$response
print(paste0("In this model run, the ecoregion is ", ecoregion," and the response variable is ",response))
```

Subset the data to only include data for the ecoregion of interest
```{r subset data to that ecoregion}

if (ecoregion == "shrubGrass") {
  # select data for the ecoregion of interest
  modDat_1 <- modDat_1 %>%
    filter(newRegion == "dryShrubGrass")
} else if (ecoregion == "forest") {
  # select data for the ecoregion of interest
  modDat_1 <- modDat_1 %>% 
    filter(newRegion %in% c("eastForest", "westForest"))
}

# remove the rows that have no observations for the response variable of interest
modDat_1 <- modDat_1[!is.na(modDat_1[,response]),]
```

## Visualize the response variable 
```{r visualize response variable}
hist(modDat_1[,response], main = paste0("Histogram of ",response),
     xlab = paste0(response))
```


## Visualize the predictor variables

The following are the candidate predictor variables for this ecoregion: 
```{r get candidate predictor variables}
if (ecoregion == "shrubGrass") {
  # select potential predictor variables for the ecoregion of interest
        prednames <-
          c(
"tmean"             , "prcp"                    ,"prcp_seasonality"        ,"prcpTempCorr"          , 
"isothermality"     , "annWatDef"               ,"sand"                    ,"coarse"                , 
"carbon"            , "AWHC"                    ,"tmin_anom"               ,"tmax_anom"             , 
"t_warm_anom"       , "prcp_wet_anom"           ,"precp_dry_anom"          ,"prcp_seasonality_anom" , 
"prcpTempCorr_anom" , "aboveFreezingMonth_anom" ,"isothermality_anom"      ,"annWatDef_anom"        , 
"annWetDegDays_anom", "VPD_mean_anom"           ,"VPD_min_anom"            ,"frostFreeDays_5_anom"   )
  
} else if (ecoregion == "forest") {
  # select potential predictor variables for the ecoregion of interest
  prednames <- 
    c(
"tmean"                 ,"prcp"               , "prcp_dry"                , "prcpTempCorr"      ,     
"isothermality"         ,"annWatDef"          , "clay"                    , "sand"              ,     
"coarse"                ,"carbon"             , "AWHC"                    , "tmin_anom"         ,     
"tmax_anom"             ,"prcp_anom"          , "prcp_wet_anom"           , "precp_dry_anom"    ,     
"prcp_seasonality_anom" ,"prcpTempCorr_anom"  , "aboveFreezingMonth_anom" , "isothermality_anom",     
"annWatDef_anom"        ,"annWetDegDays_anom" , "VPD_mean_anom"           , "VPD_max_95_anom"   ,     
"frostFreeDays_5_anom"   )
}

# subset the data to only include these predictors, and remove any remaining NAs 
modDat_1 <- modDat_1 %>% 
  select(prednames, response, newRegion, Year, Long, Lat, NA_L1NAME, NA_L2NAME) %>% 
  drop_na()

names(prednames) <- prednames
df_pred <- modDat_1[, prednames]
# 
# # print the list of predictor variables
# knitr::kable(format = "html", data.frame("Possible_Predictors" = prednames)
# ) %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 
```


```{r summary_table}
create_summary <- function(df) {
  df %>% 
    pivot_longer(cols = everything(),
                 names_to = 'variable') %>% 
    group_by(variable) %>% 
    summarise(across(value, .fns = list(mean = ~mean(.x, na.rm = TRUE), min = ~min(.x, na.rm = TRUE), 
                                        median = ~median(.x, na.rm = TRUE), max = ~max(.x, na.rm = TRUE)))) %>% 
    mutate(across(where(is.numeric), round, 4))
}

modDat_1[prednames] %>% 
  create_summary() %>% 
  knitr::kable(caption = 'summaries of possible predictor variables') %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 


# response_summary <- modDat_1 %>% 
#     dplyr::select(#where(is.numeric), -all_of(pred_vars),
#       matches(response)) %>% 
#     create_summary()
# 
# 
# kable(response_summary, 
#       caption = 'summaries of response variables, calculated using paint') %>%
# kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 

```

## Plot predictor vars against each other

```{r pred_v_pred, fig.width=10}
set.seed(12011993)
# function for colors
my_fn <- function(data, mapping, method="p", use="pairwise", ...){
  
  # grab data
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  
  # calculate correlation
  corr <- cor(x, y, method=method, use=use)
  
  # calculate colour based on correlation value
  # Here I have set a correlation of minus one to blue, 
  # zero to white, and one to red 
  # Change this to suit: possibly extend to add as an argument of `my_fn`
  colFn <- colorRampPalette(c("red", "white", "blue"), interpolate ='spline')
  fill <- colFn(100)[findInterval(corr, seq(-1, 1, length=100))]
  
  ggally_cor(data = data, mapping = mapping, size = 2.5, stars = FALSE, 
             digits = 2, colour = I("black"),...) + 
    theme_void() +
    theme(panel.background = element_rect(fill=fill))
  
}

if (run == TRUE) {
(corrPlot <- modDat_1 %>% 
  select(prednames) %>% 
  slice_sample(n = 5e4) %>% 
  #select(-matches("_")) %>% 
ggpairs( upper = list(continuous = my_fn, size = .1), lower = list(continuous = GGally::wrap("points", alpha = 0.1, size=0.1)), progress = FALSE))
    base::saveRDS(corrPlot, paste0("../ModelFitting/models/", response, "_",ecoregion, "_corrPlot.rds"))
  
  } else {
    # corrPlot <- readRDS(paste0("../ModelFitting/models/", response, "_",ecoregion, "_corrPlot.rds"))
    # (corrPlot)
    print(c("See previous correlation figures"))
  }
```

## Predictor variables compared to binned response variables

```{r climVar_boxplots, fig.height=9, fig.width=8}
set.seed(12011993)
# vector of name of response variables
vars_response <- response

# longformat dataframes for making boxplots
df_sample_plots <-  modDat_1  %>% 
  slice_sample(n = 5e4) %>% 
   rename(response = all_of(response)) %>% 
  mutate(response = case_when(
    response <= .25 ~ ".25", 
    response > .25 & response <=.5 ~ ".5", 
    response > .5 & response <=.75 ~ ".75", 
    response >= .75  ~ "1", 
  )) %>% 
  select(c(response, prednames)) %>% 
  tidyr::pivot_longer(cols = unname(prednames), 
               names_to = "predictor", 
               values_to = "value"
               )  
 

  ggplot(df_sample_plots, aes_string(x= "response", y = 'value')) +
  geom_boxplot() +
  facet_wrap(~predictor , scales = 'free_y') + 
  ylab("Predictor Variable Values") + 
    xlab(response)

```

## Standardize the predictor variables for the model-fitting process
```{r scaling predictors, fig.height=7, fig.width=10}
modDat_1_s <- modDat_1 %>% 
  mutate(across(all_of(prednames), base::scale, .names = "{.col}_s")) 
names(modDat_1_s) <- c(names(modDat_1),
                       paste0(prednames, "_s")
                       )
  
scaleFigDat_1 <- modDat_1_s %>% 
  select(c(Long, Lat, Year, prednames)) %>% 
  pivot_longer(cols = all_of(names(prednames)), 
               names_to = "predNames", 
               values_to = "predValues_unScaled")
scaleFigDat_2 <- modDat_1_s %>% 
  select(c(Long, Lat, Year,paste0(prednames, "_s"))) %>% 
  pivot_longer(cols = all_of(paste0(prednames, "_s")), 
               names_to = "predNames", 
               values_to = "predValues_scaled", 
               names_sep = ) %>% 
  mutate(predNames = str_replace(predNames, pattern = "_s$", replacement = ""))

scaleFigDat_3 <- scaleFigDat_1 %>% 
  left_join(scaleFigDat_2)

ggplot(scaleFigDat_3) + 
  facet_wrap(~predNames, scales = "free") +
  geom_histogram(aes(predValues_unScaled), fill = "lightgrey", col = "darkgrey") + 
  geom_histogram(aes(predValues_scaled), fill = "lightblue", col = "blue") +
  xlab ("predictor variable values") + 
  ggtitle("Comparing the distribution of unscaled (grey) to scaled (blue) predictor variables")
  
```

# Model Fitting

## Visualize the level 2 ecoregions and how they differ across environmental space
```{r Visualize ecoregion distribution, fig.width = 14, fig.height=10, message=FALSE}
## visualize the variation between groups across environmental space
## make data into spatial format
modDat_1_sf <- modDat_1 %>% 
  st_as_sf(coords = c("Long", "Lat"), crs = st_crs("PROJCRS[\"unnamed\",\n    BASEGEOGCRS[\"unknown\",\n        DATUM[\"unknown\",\n            ELLIPSOID[\"Spheroid\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1,\n                    ID[\"EPSG\",9001]]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]]],\n    CONVERSION[\"Lambert Conic Conformal (2SP)\",\n        METHOD[\"Lambert Conic Conformal (2SP)\",\n            ID[\"EPSG\",9802]],\n        PARAMETER[\"Latitude of false origin\",42.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-100,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",25,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",60,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"northing\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]"))


## do a pca of climate across level 2 ecoregions
pca <- prcomp(modDat_1_s[,paste0(prednames, "_s")])
library(factoextra)
(fviz_pca_ind(pca, habillage = modDat_1_s$NA_L2NAME, label = "none", addEllipses = TRUE, ellipse.level = .95, ggtheme = theme_minimal(), alpha.ind = .1))

if(ecoregion == "shrubGrass") {
  print("We'll combine the 'Mediterranean California' and 'Western Sierra Madre Piedmont' ecoregions (into 'Mediterranean Piedmont'). We'll also combine `Tamaulipas-Texas semiarid plain' and 'South Central semiarid prairies' ecoregions (into (`Semiarid plain and prairies`)" )
  
  modDat_1_s[modDat_1_s$NA_L2NAME %in% c("MEDITERRANEAN CALIFORNIA", "WESTERN SIERRA MADRE PIEDMONT"), "NA_L2NAME"] <- "MEDITERRANEAN PIEDMONT"
  modDat_1[modDat_1$NA_L2NAME %in% c("MEDITERRANEAN CALIFORNIA", "WESTERN SIERRA MADRE PIEDMONT"), "NA_L2NAME"] <- "MEDITERRANEAN PIEDMONT"
  
  modDat_1_s[modDat_1_s$NA_L2NAME %in% c("TAMAULIPAS-TEXAS SEMIARID PLAIN", "SOUTH CENTRAL SEMIARID PRAIRIES"), "NA_L2NAME"] <- "SEMIARID PLAIN AND PRAIRIES"
  modDat_1[modDat_1$NA_L2NAME %in% c("TAMAULIPAS-TEXAS SEMIARID PLAIN", "SOUTH CENTRAL SEMIARID PRAIRIES"), "NA_L2NAME"] <- "SEMIARID PLAIN AND PRAIRIES"
}
# make a table of n for each region
modDat_1 %>% 
  group_by(NA_L2NAME) %>% 
  dplyr::summarize("Number_Of_Observations" = length(NA_L2NAME)) %>% 
  rename("Level_2_Ecoregion" = NA_L2NAME)%>% 
  kable() %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 

```

## Then, look at the spatial distribution and environmental characteristics of the grouped ecoregions 
```{r fig.width = 14, fig.height=10, message=FALSE}
# download map info for visualization
us_states <- suppressMessages(tigris::states())


cropped_states <- suppressMessages(us_states %>%
  dplyr::filter(NAME!="Hawaii") %>%
  dplyr::filter(NAME!="Alaska") %>%
  dplyr::filter(NAME!="Puerto Rico") %>%
  dplyr::filter(NAME!="American Samoa") %>%
  dplyr::filter(NAME!="Guam") %>%
  dplyr::filter(NAME!="Commonwealth of the Northern Mariana Islands") %>%
  dplyr::filter(NAME!="United States Virgin Islands") %>%

  sf::st_sf() %>%
  sf::st_transform(sf::st_crs(modDat_1_sf))) #%>%
  #sf::st_crop(sf::st_bbox(modDat_1_sf)+c(-1,-1,1,1))

map1 <- ggplot() +
  geom_sf(data=cropped_states,fill='white') +
  geom_sf(data=modDat_1_sf,aes(fill=as.factor(NA_L2NAME)),linewidth=0.5,alpha=0.5) +
  geom_point(data=modDat_1,alpha=0.5, 
             aes(x = Long, y = Lat, color=as.factor(NA_L2NAME))) +
  #scale_fill_okabeito() +
  #scale_color_okabeito() +
 # theme_default() +
  theme(legend.position = 'none') +
  labs(title = "Level 2 Ecoregions as spatial blocks")

hull <- modDat_1_sf %>%
  ungroup() %>%
  group_by(NA_L2NAME) %>%
  slice(chull(tmean, prcp))

plot1<-ggplot(data=modDat_1_sf,aes(x=tmean,y=prcp)) +
  geom_polygon(data = hull, alpha = 0.25,aes(fill=NA_L2NAME) )+
  geom_point(aes(group=NA_L2NAME,color=NA_L2NAME),alpha=0.25) +
  theme_minimal() + xlab("Annual Average T_mean - long-term average") +
  ylab("Annual Average Precip - long-term average") #+
  #scale_color_okabeito() +
  #scale_fill_okabeito()

plot2<-ggplot(data=modDat_1_sf %>%
                pivot_longer(cols=tmean:prcp),
              aes(x=value,group=name)) +
  # geom_polygon(data = hull, alpha = 0.25,aes(fill=fold) )+
  geom_density(aes(group=NA_L2NAME,fill=NA_L2NAME),alpha=0.25) +
  theme_minimal() +
  facet_wrap(~name,scales='free')# +
  #scale_color_okabeito() +
  #scale_fill_okabeito()
 
library(patchwork)
(combo <- (map1+plot1)/plot2) 

```

## Fit a global model with all of the data 
 Within the training set, use level 2 ecoregion for cross-validation to tune lambda in the LASSO model 
```{r fit global model, echo = FALSE}
  # Fit model 
  # penalty values
  lambdas <- 10^seq(0 ,-7, length.out = 200)  # sequence of penalties to test
 
  
  # make model matrix (which includes transformations and interactions)
  ## only include interactions within and between climate and anomaly variables, and within and between soils variables

 ## break predictors into climate, weather, and soils 
  prednames_clim <- prednames[which(prednames %in% c("tmin" , "tmax",  "tmean",
     "prcp", "t_warm" , "t_cold" , "prcp_wet" , "prcp_dry" , "prcp_seasonality", "prcpTempCorr", "abvFreezingMonth" , "isothermality" , "annWatDef" , "annWetDegDays",  "VPD_mean",  "VPD_max",  "VPD_min" , "VPD_max_95", "annWatDef_95", "annWetDegDays_5",  "frostFreeDays_5", "frostFreeDays"))]
  prednames_weath <- prednames[which(prednames %in% c("tmean_anom", "tmin_anom",  "tmax_anom", 
    "prcp_anom",      "t_warm_anom" , "t_cold_anom",  "prcp_wet_anom" , "precp_dry_anom" ,  "prcp_seasonality_anom" ,  "prcpTempCorr_anom" , "aboveFreezingMonth_anom",  "isothermality_anom" ,  "annWatDef_anom", "annWetDegDays_anom",  "VPD_mean_anom" , "VPD_min_anom" , "VPD_max_anom", "VPD_max_95_anom", "annWatDef_95_anom" , "annWetDegDays_5_anom",  "frostFreeDays_5_anom" , "frostFreeDays_anom"))]
  prednames_soils <- prednames[which(prednames %in% c( "soilDepth" , "clay", 
     "sand" , "coarse" ,  "carbon", "AWHC"))]
  
  # add "_s" to scaled variables to all prednames
  prednames_clim <- paste0(prednames_clim, "_s")
  prednames_weath <- paste0(prednames_weath, "_s")
  prednames_soils <- paste0(prednames_soils, "_s")
  
  # get a text string of possible interactions between climate and weather variables
  combos <- data.frame(gtools::permutations(n = length(c(prednames_clim, prednames_weath)),
                                            r = 2,
                                            v = c(prednames_clim, prednames_weath)))
  
  combos$interactions <- paste0(combos$X1, ":", combos$X2)
  
  
  # get a text string of possible interactions between soils variables
  combos_2 <- data.frame(gtools::permutations(n = length(c(prednames_soils)),
                                            r = 2,
                                            v = c(prednames_soils)))
  
  combos_2$interactions <- paste0(combos_2$X1, ":", combos_2$X2)
  
  # make text string of all possible interactions
  int_string <-str_flatten(c(combos$interactions, combos_2$interactions), collapse = " + ")
  
  # make text string of transformations (log, squared)
  transform_string <- str_flatten(c(# add square root term 
                                    paste0("I(log(",prednames_clim,"+20))"), 
                                    paste0("I(log(",prednames_weath,"+20))"),
                                    paste0("I(log(",prednames_soils,"+20))"),
                                    
                                    paste0("I(sqrt(",prednames_clim,"+20))"), 
                                    paste0("I(sqrt(",prednames_weath,"+20))"),
                                    paste0("I(sqrt(",prednames_soils,"+20))"),
                                    
                                    paste0("I(",prednames_clim,"^2)"), 
                                    paste0("I(",prednames_weath,"^2)"),
                                    paste0("I(",prednames_soils,"^2)"))
                                    , collapse = " + ")
              
  # get a text string of the possible untransformed model terms
  mod_string <- paste(response, "~", paste0(prednames, "_s", collapse = "+"))
  
  # make a text string of the model formula w/ interactions and transformations included
  modWithInteractions <- paste(mod_string, "+",int_string , "+", transform_string
                               )
   
  # now, make a model formula object
  f <- as.formula(modWithInteractions)
  
  # transform dataframe to model matrix
  X <- model.matrix(f, modDat_1_s)
  # get response variable
  y <- as.matrix(modDat_1_s[,response])
  
  # split into training and test sets
  # hold out several years ()
  # test_eco <- unique(modDat_1_s$NA_L2NAME)[7]
  # 
  # get the ecoregions for training lambda
  train_eco <- modDat_1_s$NA_L2NAME#[train]
  
  # Fit model -----------------------------------------------

  # # don't penalize previous year's abundance or meadow ID
  # pen_facts <- rep(1, times = ncol(X))
  # logNtId <- which(colnames(X) == "logNt")
  # meadaId <- grep("meada",colnames(X))
  # pen_facts[c(logNtId,meadaId)] <- 0  
  
  # specify leave-one-year-out cross-validation
  my_folds <- as.numeric(as.factor(train_eco))
 
  #thresh <- 1e-15
  
  if (run == TRUE) {
    fit <- cv.glmnet(
    x = X[,2:ncol(X)], 
    y = y, 
    family = stats::Gamma(link = "log"),
    alpha = 1,  # 0 == ridge regression, 1 == lasso, 0.5 ~~ elastic net
    lambda = lambdas, 
    type.measure="mse",
    #penalty.factor = pen_facts,
    foldid = my_folds,
    #thresh = thresh,
    standardize = FALSE ## scales variables prior to the model sequence... coefficients are always returned on the original scale
    )
    base::saveRDS(fit, paste0("../ModelFitting/models/", response, "_globalLASSOmod_gammaLogLink_",ecoregion, ".rds"))
  
  } else {
    fit <- readRDS(paste0("../ModelFitting/models/", response, "_globalLASSOmod_gammaLogLink_",ecoregion, ".rds"))
  }
  
  
  # save the minimum lambda
  best_lambda <- fit$lambda.min
  print(fit)
  # look at CV score vs penalty plot
  plot(log(fit$lambda),fit$cvm,main= paste0("comparing cross-validation score against log(lambda) \n best log(lambda) = ", round(log(best_lambda),3)), ylim = c(0, max(fit$cvm)))
  # add points for the # of coefficients for each lambda
  points(log(fit$lambda), fit$nzero, col = "red")
  
  ## get the coefficients for the minimum lambda (compare to the lambda that is 1se from the min. )
  bestLambda_coef <- glmnet:::coef.cv.glmnet(fit, s = best_lambda )
  seLambda_coef <- glmnet:::coef.cv.glmnet(fit, s = fit$lambda.1se )
  
  ## predict on the test data
  # lasso model predictions with the optimal lambda
  optimal_pred <-  glmnet:::predict.cv.glmnet(fit, newx=X[,2:ncol(X)], s= best_lambda, type = "response")
  optimal_pred_1se <-  glmnet:::predict.cv.glmnet(fit, newx=X[,2:ncol(X)], s= fit$lambda.1se, type = "response")
    null_fit <- glm(data = as.data.frame(X[,paste0(prednames, "_s")]), formula = y ~ 1, family = stats::Gamma(link = "log"))
  null_pred <- predict(null_fit, newdata = as.data.frame(X), type = "response"
                       )

  # save data
  fullModOut <- list(
    "modelObject" = fit,
    "nullModelObject" = null_fit,
    "modelPredictions" = data.frame(#ecoRegion_holdout = rep(test_eco,length(y)),
      obs=y,
                    pred_opt=optimal_pred[,1], 
                    pred_opt_se = optimal_pred_1se[,1],
                    pred_null=null_pred#,
                    #pred_nopenalty=nopen_pred
                    ))
  
  
# calculate correlations between null and optimal model 
my_cors <- c(cor(optimal_pred[,1], y),
             cor(optimal_pred_1se[,1], y), 
            cor(null_pred, y)
            )

# calculate mse between null and optimal model 
my_mse <- c(mean((fullModOut$modelPredictions$pred_opt -  y)^2) ,
            mean((fullModOut$modelPredictions$pred_opt_se -  y)^2) ,
            mean((fullModOut$modelPredictions$pred_null - y)^2)#,
            #mean((obs_pred$pred_nopenalty - obs_pred$obs)^2)
            )

plot(main = "A rough comparison of observed and model-predicted values",
  y = fullModOut$modelPredictions$obs, x = X[,2], xlab = c("tmean"), ylab = (paste(response)),
     ylim = c(-25, 170)) # observed values
points(y = fullModOut$modelPredictions$pred_opt, x = X[,2], col = "red") ## predictions w/ the CV model
points(y = fullModOut$modelPredictions$pred_opt_se, x = X[,2], col = "green") ## predictions w/ the CV model
points(y = fullModOut$modelPredictions$pred_null, x = X[,2], col = "blue") ## predictions with the null model
legend(x = 18, y = 160, legend = c("Obs. values", "Preds - best lambda", "Preds - ok lambda", "Preds - null mod."), 
       col = c("black", "red", "green", "blue"), pch = 1, cex = .7)
#points(y = pred_glm, x = X_test[,2], col = "green") ## predictions with the GLM 
#points(y = optimal_pred_bestLambda, x = X_test[,2], col = "orange") ## predictions with the best-lambda glmnet model
```

The internal cross-validation process to fit the global LASSO model identified an optimal lambda value (regularization parameter) of `r{print(best_lambda)}`. The following coefficients were kept in the model: 
```{r coefficients in the global model}
# the coefficient matrix from the 'best model' -- find and print those coefficients that aren't 0 in a table
mat <- as.matrix(coef(fit, s = best_lambda)) 
mat2 <- mat[mat[,1] != 0,]
# the coefficient matrix from the 'best lambda model' -- find and print those coefficients that aren't 0 in a table
#mat3 <- as.matrix(coef(fit_bestLambda)) 
#mat4 <- mat[mat[,1] > 0,]

globModTerms <- data.frame("Coefficient_Name" = names(mat2), 
                   "Value_From_LASSO_with_internal_CV" = unname(mat2)#, 
                   #"Coefficient_Name" = names(fit2$coefficients),
                   #"Value_From_glm" = unname(fit2$coefficients),
                   #"Coefficient_Name"= names(mat4),
                   #"Value_From_LASSO_withBestLambda" = unname(mat4)
                   )

kable(globModTerms, col.names = c("Coefficient Name", "Value from LASSO")
      ) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 
```

# Visualizations of Model Predictions and Residuals
## observed vs. predicted values

### Predicting on the data

```{r predict on data used for fitting}

  # create prediction for each each model
# (i.e. for each fire proporation variable)
predict_by_response <- function(mod, df) {
  df_out <- df
  response_name <- paste0(response, "_pred")
  df_out <- df_out %>% cbind(predict(mod, newx= df_out, s="lambda.min", type = "response"))
   colnames(df_out)[ncol(df_out)] <- response_name
  return(df_out)
}

pred_glm1 <- predict_by_response(fit, X[,2:ncol(X)])

# add back in true y values
pred_glm1 <- pred_glm1 %>% 
  cbind( data.frame("y" = y))
# rename the true response column to not be 'y_test' 
colnames(pred_glm1)[which(colnames(pred_glm1) == "y")] <- paste(response)

# add back in lat/long data 
pred_glm1 <- pred_glm1 %>% 
  cbind(modDat_1_s[,c("Long", "Lat", "Year")])

pred_glm1$resid <- pred_glm1[,response] - pred_glm1[,paste0(response, "_pred")]
pred_glm1$extremeResid <- NA
pred_glm1[pred_glm1$resid > 70 | pred_glm1$resid < -70,"extremeResid"] <- 1

# plot(x = pred_glm1[,response],
#      y = pred_glm1[,paste0(response, "_pred")],
#      xlab = "observed values", ylab = "predicted values")
# points(x = pred_glm1[!is.na(pred_glm1$extremeResid), response],
#        y = pred_glm1[!is.na(pred_glm1$extremeResid), paste0(response, "_pred")],
#        col = "red")
```

### Maps of Observations, Predictions, and Residuals=

Observations across the temporal range of the dataset
```{r Residual maps5, fig.height = 7, fig.width = 9, }

pred_glm1 <- pred_glm1 %>% 
  mutate(resid = .[[response]] - .[[paste0(response,"_pred")]]) 

# rasterize
# get reference raster
test_rast <-  rast("../../../Data_raw/dayMet/rawMonthlyData/orders/70e0da02b9d2d6e8faa8c97d211f3546/Daymet_Monthly_V4R1/data/daymet_v4_prcp_monttl_na_1980.tif") %>% 
  terra::aggregate(fact = 8, fun = "mean")

# rasterize data
plotObs <- pred_glm1 %>% 
         drop_na(paste(response)) %>% 
  #slice_sample(n = 5e4) %>%
  terra::vect(geom = c("Long", "Lat")) %>% 
  terra::set.crs(crs(test_rast)) %>% 
  terra::rasterize(y = test_rast, 
                   field = response, 
                   fun = mean) %>% 
  #terra::aggregate(fact = 2, fun = mean, na.rm = TRUE) %>% 
  terra::crop(ext(-1950000, 1000000, -1800000, 1000000))

# make figures
ggplot() +
geom_spatraster(data = plotObs) + 
  geom_sf(data=cropped_states %>% st_transform(crs = st_crs(test_rast)),fill=NA ) +
labs(title = paste0("Observations of ",s, " in the ",ecoregion, " ecoregion"),
     subtitle = paste0(paste0(globModTerms$Coefficient_Name[1:5], collapse = " + "), " + \n", paste0(globModTerms$Coefficient_Name[6:length(globModTerms$Coefficient_Name)], collapse = " + "), collapse = " + ")) +
  scale_fill_gradient2(low = "brown",
                       mid = "wheat" ,
                       high = "darkgreen" , 
                       midpoint = 0,   na.value = "white")

```

Predictions across the temporal range of the dataset
```{r Residual maps, fig.height = 7, fig.width = 9, }

# rasterize data
plotPred <- pred_glm1 %>% 
         drop_na(paste0(response,"_pred")) %>% 
  #slice_sample(n = 5e4) %>%
  terra::vect(geom = c("Long", "Lat")) %>% 
  terra::set.crs(crs(test_rast)) %>% 
  terra::rasterize(y = test_rast, 
                   field = paste0(response,"_pred"), 
                   fun = mean) %>% 
  #terra::aggregate(fact = 2, fun = mean, na.rm = TRUE) %>% 
  terra::crop(ext(-1950000, 1000000, -1800000, 1000000))

# get the point location of those predictions that are > 100
highPred_points <- pred_glm1 %>% 
  filter(.[[paste0(response,"_pred")]] > 100) %>% 
  terra::vect(geom = c("Long", "Lat")) %>% 
  terra::set.crs(crs(test_rast)) 

# make figures
ggplot() +
geom_spatraster(data = plotPred) + 
  geom_sf(data=cropped_states %>% st_transform(crs = st_crs(test_rast)),fill=NA ) +
  geom_sf(data = highPred_points, col = "red") +
labs(title = paste0("Predictions from the fitted model of ",s, " in the ",ecoregion, " ecoregion"),
     subtitle = paste0("Red points indicate predictions >100 \n", paste0(globModTerms$Coefficient_Name[1:5], collapse = " + "), " + \n", paste0(globModTerms$Coefficient_Name[6:length(globModTerms$Coefficient_Name)], collapse = " + "), collapse = " + ")) +
  scale_fill_gradient2(low = "wheat",
                       mid = "darkgreen",
                       high = "red" , 
                       midpoint = 100,   na.value = "white",
                       limits = c(0,100))
```

Residuals across the entire temporal extent of the dataset
```{r Residual maps2, fig.height = 12, fig.width = 9, warning = FALSE }
# rasterize data
plotResid_rast <- pred_glm1 %>% 
         drop_na(resid) %>% 
  #slice_sample(n = 5e4) %>%
  terra::vect(geom = c("Long", "Lat")) %>% 
  terra::set.crs(crs(test_rast)) %>% 
  terra::rasterize(y = test_rast, 
                   field = "resid", 
                   fun = mean) %>% 
  #terra::aggregate(fact = 2, fun = mean, na.rm = TRUE) %>% 
  terra::crop(ext(-1950000, 1000000, -1800000, 1000000))

# make figures
map <- ggplot() +
geom_spatraster(data = plotResid_rast) + 
  geom_sf(data=cropped_states %>% st_transform(crs = st_crs(test_rast)),fill=NA ) +
labs(title = paste0("Resids. (obs. - pred.) from Grass/shrub ecoregion-wide model of ",s),
     subtitle = paste0(paste0(globModTerms$Coefficient_Name[1:5], collapse = " + "), " + \n", paste0(globModTerms$Coefficient_Name[6:length(globModTerms$Coefficient_Name)], collapse = " + "), collapse = " + ")) +
  scale_fill_gradient2(low = "red",
                       mid = "white" ,
                       high = "blue" , 
                       midpoint = 0,   na.value = "white"#,
                       #limits = c(-100,100)
                       )
hist <- ggplot(pred_glm1) + 
  geom_histogram(aes(resid), fill = "lightgrey", col = "darkgrey") + 
  geom_text(aes(x = min(resid)*.9, y = 1500, label = paste0("min = ", round(min(resid),2)))) +
  geom_text(aes(x = max(resid)*.9, y = 1500, label = paste0("max = ", round(max(resid),2))))

library(ggpubr)
ggarrange(map, hist, heights = c(3,1), ncol = 1)

```

### Quantile plots

Binning predictor variables into "Deciles" (actually percentiles) and looking at the mean
predicted probability for each percentile. The use of the word Decentiles
is just a legacy thing (they started out being actual Percentiles)


```{r}
var_prop_pred <- paste0(response, "_pred")
response_vars <- c(response, var_prop_pred)

prednames_fig <- paste(str_split(globModTerms$Coefficient_Name, ":", simplify = TRUE)) 
prednames_fig <- str_replace(prednames_fig, "I\\(", "")
prednames_fig <- str_replace(prednames_fig, "\\^2\\)", "")
prednames_fig <- unique(prednames_fig[prednames_fig>0])
pred_glm1_deciles <- predvars2deciles(pred_glm1,
                                      response_vars = response_vars,
                                        pred_vars = prednames_fig)

```


Here is a quick version of LOESS curves fit to raw data (to double-check the quantile plot calculations)
```{r, fig.height= 12, fig.width = 12}

pred_glm1 %>%
  select(all_of(c(prednames_fig, response_vars))) %>%
  pivot_longer(cols = prednames_fig)  %>%
  ggplot() +
  facet_wrap(~name, scales = "free") +
  geom_point(aes(x = value, y = TotalHerbaceousCover), col = "darkblue", alpha = .1)  + # observed values
  geom_point(aes(x = value, y = TotalHerbaceousCover_pred), col = "lightblue", alpha = .1) + # model-predicted values
  geom_smooth(aes(x = value, y = TotalHerbaceousCover), col = "black", se = FALSE) +
  geom_smooth(aes(x = value, y = TotalHerbaceousCover_pred), col = "brown", se = FALSE)

```

Below are the actual quantile plots 
(note that the predictor variables are scaled) 
```{r Percentile_plot, fig.height= 12, fig.width = 12, warning=FALSE}
# publication quality version
g3 <- decile_dotplot_pq(pred_glm1_deciles, response = response) + ggtitle("Decile Plot")

if(!hmod) {
# obs/pred inset
g4 <- add_dotplot_inset(g3, pred_glm1_deciles)
} else {
  g4 <- g3
}

  
if(save_figs) {
  png(paste0("figures/quantile_plots/quantile_plot_", s,  "_",ecoregion,".png"), 
     units = "in", res = 600, width = 5.5, height = 3.5 )
    print(g4)
  dev.off()
}

g4
```

### Deciles Filtered 

20th and 80th percentiles for each climate variable

```{r}
df <- pred_glm1[, prednames_fig] #%>% 
  #mutate(MAT = MAT - 273.15) # k to c
quantiles <- map(df, quantile, probs = c(0.2, 0.8), na.rm = TRUE)
```

Filtered 'Decile' plots of data. These plots show each vegetation variable,
but only based on data that falls into the upper and lower two deciles of
each predictor variable. 

```{r glm_deciles_filtered, fig.height = 24, fig.width = 24, message = FALSE}

pred_glm1_deciles_filt <- predvars2deciles( pred_glm1, 
                         response_vars = response_vars,
                         pred_vars = prednames_fig,
                         filter_var = TRUE,
                         filter_vars = prednames_fig) 

decile_dotplot_filtered_pq(pred_glm1_deciles_filt, xvars = prednames_fig)
#decile_dotplot_filtered_pq(pred_glm1_deciles_filt)

```

Filtered quantile figure with middle 2 deciles also shown

```{r fig.height = 24, fig.width = 24, eval = TRUE}
pred_glm1_deciles_filt_mid <- predvars2deciles(pred_glm1, 
                         response_vars = response_vars,
                         pred_vars = prednames_fig,
                         filter_vars = prednames_fig,
                         filter_var = TRUE,
                         add_mid = TRUE)

g <- decile_dotplot_filtered_pq(df = pred_glm1_deciles_filt_mid, xvars = prednames_fig)
g

if(save_figs) {x
jpeg(paste0("figures/quantile_plots/quantile_plot_filtered_mid_v1", s, ".jpeg"),
     units = "in", res = 600, width = 5.5, height = 6 )
  g 
dev.off()
}
```

# Cross-validation 
## Use terms from global model to re-fit and predict on different held out regions
## Figures show residuals for each of the models fit to held-out ecoregions 
These models were fit to six ecoregions, and then predict on the indicated heldout ecoregion

```{r cross-validation, fig.height = 7, fig.width = 9 }
## code from Tredennick et al. 2020
# try each separate level II ecoregion as a test set
# make a list to hold output data
outList <- vector(mode = "list", length = length(sort(unique(modDat_1$NA_L2NAME))))
# obs_pred <- data.frame(ecoregion = character(),obs = numeric(),
#                        pred_opt = numeric(), pred_null = numeric()#,
#                        #pred_nopenalty = numeric()
#                        )

## get the model specification from the global model
mat <- as.matrix(glmnet::coef.glmnet(fit, s = "lambda.min"))
mat2 <- mat[mat[,1] != 0,]

f <- as.formula(paste0(response, " ~ ", paste0(names(mat2)[2:length(names(mat2))], collapse = " + ")))

X_cv <- model.matrix(object = f, data = modDat_1_s)
# get response variable
y_cv <- as.matrix(modDat_1_s[,response])


  # now, loop through so with each iteration, a different ecoregion is held out
 for(i_eco in sort(unique(modDat_1_s$NA_L2NAME))){

  # split into training and test sets
  test_eco <- i_eco
  print(test_eco)
  # identify the rowID of observations to be in the training and test datasets
  train <- which(modDat_1_s$NA_L2NAME!=test_eco) # data for all ecoregions that aren't 'i_eco'
  test <- which(modDat_1_s$NA_L2NAME==test_eco) # data for the ecoregion that is 'i_eco'

  trainDat_all <- modDat_1_s[train,]
  testDat_all <- modDat_1_s[test,]

  # get the model matrices for input and response variables for cross validation model specification
  X_train <- as.matrix(X_cv[train,])
  X_test <- as.matrix(X_cv[test,])

  y_train <- modDat_1_s[train,response]
  y_test <- modDat_1_s[test,response]
  
  # get the model matrices for input and response variables for original model specification
  X_train_glob <- as.matrix(X[train,])
  X_test_glob <- as.matrix(X[test,])

  y_train_glob <- modDat_1_s[train,response]
  y_test_glob <- modDat_1_s[test,response]

  train_eco <- modDat_1_s$NA_L2NAME[train]

  # Fit model using glm-----------------------------------------------
#
#   # specify leave-one-year-out cross-validation
#   my_folds <- as.numeric(as.factor(train_eco))
#
  # fit_i <- cv.glmnet(
  #   x = X_train[,2:ncol(X_train)],
  #   y = y_train,
  #   #family = gaussian,
  #   family = stats::Gamma(link = "log"),
  #   alpha = 1,  # 0 == ridge regression, 1 == lasso, 0.5 ~~ elastic net
  #   lambda = lambdas,
  #   type.measure="mse",
  #   #penalty.factor = pen_facts,
  #   foldid = my_folds,
  #   standardize = TRUE
  # )

  ## just try a regular glm w/ the components from the global model
  fit_i <- glm(data = trainDat_all, formula = f, 
               family =  stats::Gamma(link = "log"))

    coef(fit_i)
    
  # lasso model predictions with the optimal lambda
  optimal_pred <- predict(fit_i, newdata= testDat_all, type = "response"
                          )
  # null model and predictions
  # the "null" model in this case is the global model
  # predict on the test data for this iteration w/ the global model 
  null_pred <- glmnet:::predict.cv.glmnet(fit, newx = X_test_glob[,2:ncol(X_test_glob)], type = "response", s=min(fit$lambda))

  # save data
  tmp <- data.frame(ecoRegion_holdout = rep(test_eco,length(y_test)),obs=y_test,
                    pred_opt=optimal_pred, pred_null=null_pred#,
                    #pred_nopenalty=nopen_pred
                    ) %>%
    cbind(testDat_all)
  # put output into a list
  tmpList <- list("testRegion" = i_eco,
    "modelObject" = fit_i,
       "modelPredictions" = tmp)

  # save model outputs
  outList[[which(sort(unique(modDat_1_s$NA_L2NAME)) == i_eco)]] <- tmpList
  # save one example of model fits
  # if(i_eco==sort(unique(modDat_1_s$NA_L2NAME))[1]){
  #   # save model object
  #   saveRDS(fit,file="./ModelResults/glmnet_fit.RDS")
  #
  #   #saveRDS(nopen,file="./ModelResults/nopenalty_fit.RDS")
  # }
 }

#
# # calculate correlations between null and optimal model
# my_cors <- c(cor(outList[[i]]$modelPredictions$pred_opt, outList[[i]]$modelPredictions$obs), # correlation of LOO model's predictions w/ the observations
#             cor(outList[[i]]$modelPredictions$s1, outList[[i]]$modelPredictions$obs)#, # correlations of LOO model's predictions w/ global model's predictions
#             #cor(obs_pred$pred_nopenalty,obs_pred$obs)
#             )
# 
# # calculate mse between null and optimal model
# my_mse <- c(mean((outList[[i]]$modelPredictions$pred_opt - outList[[i]]$modelPredictions$obs)^2) ,
#             mean((outList[[i]]$modelPredictions$s1 - outList[[i]]$modelPredictions$obs)^2)#,
#             #mean((obs_pred$pred_nopenalty - obs_pred$obs)^2)
#             )

# # print results
# names(my_cors) <- names(my_mse) <- c("optimal","null"#,"no penalty"
#                                      )
# print(my_cors)
# print(my_mse)
#
# # save obs vs pred results
# write.csv(obs_pred,"./ModelResults/obs_vs_pred.csv",row.names=F)

for (i in 1:length(unique(modDat_1_s$NA_L2NAME))) {
  holdoutRegion <- outList[[i]]$testRegion
  predictionData <- outList[[i]]$modelPredictions
  modTerms <- as.matrix(coef(outList[[i]]$modelObject)) %>%
    as.data.frame() %>%
    filter(V1!=0) %>%
    rownames()

  # calculate residuals
  predictionData <- predictionData %>%
  mutate(resid = .[["obs"]] - .[["pred_opt"]] ,
         resid_globMod = .[["obs"]]  - .[["s1"]])


# rasterize
# use 'test_rast' from earlier

  # rasterize data
plotObs <- predictionData %>%
         drop_na(paste(response)) %>%
  #slice_sample(n = 5e4) %>%
  terra::vect(geom = c("Long", "Lat")) %>%
  terra::set.crs(crs(test_rast)) %>%
  terra::rasterize(y = test_rast,
                   field = "resid",
                   fun = mean) %>%
  #terra::aggregate(fact = 2, fun = mean, na.rm = TRUE) %>%
  terra::crop(ext(-1950000, 1000000, -1800000, 1000000))


# make figures
# make histogram
hist_i <- ggplot(predictionData) +
  geom_histogram(aes(resid_globMod), col = "darkgrey", fill = "lightgrey") +
  xlab(c("Residuals (obs. - pred.)"))
# make map
map_i <-  ggplot() +
geom_spatraster(data = plotObs) +
  geom_sf(data=cropped_states %>% st_transform(crs = st_crs(test_rast)),fill=NA ) +
labs(title = paste0("Residuals (obs. - pred.) for predictions of \n", holdoutRegion, " \n from a model fit to other ecoregions"),
     subtitle = paste0(response, " ~ ", paste0( modTerms, collapse = " + "))) +
  scale_fill_gradient2(low = "red",
                       mid = "white" ,
                       high = "blue" ,
                       midpoint = 0,   na.value = "white",
                       limits = c(-100, 100))

 assign(paste0("residPlot_",holdoutRegion),
   value = ggarrange(map_i, hist_i, heights = c(3,1), ncol = 1)
)

}


ggarrange(
  get(paste0("residPlot_", unique(modDat_1_s$NA_L2NAME))[1]),
  get(paste0("residPlot_", unique(modDat_1_s$NA_L2NAME))[2]),
  get(paste0("residPlot_", unique(modDat_1_s$NA_L2NAME))[3]),
  get(paste0("residPlot_", unique(modDat_1_s$NA_L2NAME))[4]),
  get(paste0("residPlot_", unique(modDat_1_s$NA_L2NAME))[5]),
  get(paste0("residPlot_", unique(modDat_1_s$NA_L2NAME))[6]),
  get(paste0("residPlot_", unique(modDat_1_s$NA_L2NAME))[7]),
  #get(paste0("residPlot_", unique(modDat_1_s$NA_L2NAME))[8]),
  #get(paste0("residPlot_", unique(modDat_1_s$NA_L2NAME))[9]),
  ncol = 2, common.legend = TRUE
)

```



# Save output

```{r save_output}
# # glm models
# #mods2save <- butcher::butcher(mod_glmFinal) # removes some model components so the saved object isn't huge
# 
# #mods2save$formula <- best_form
# #mods2save$pred_vars_inter <- pred_vars_inter # so have interactions
# #n <- nrow(df_sample)
# #mods2save$data_rows <- n
# 
# 
# if(!test_run) {
#   saveRDS(mods2save, 
#         paste0("./models/glm_beta_model_CONUSwide_", s, "_n", n, 
#         #sample_group, 
#         ".RDS"))
#   if (byRegion == TRUE) {
#     ## western forests
#      saveRDS(mods2save_WF, 
#         paste0("./models/glm_beta_model_WesternForests_", s, "_n", n, 
#         #sample_group, 
#         ".RDS"))
#     ## eastern forests
#      saveRDS(mods2save_EF, 
#         paste0("./models/glm_beta_model_EasternForests_", s, "_n", n, 
#         #sample_group, 
#         ".RDS"))
#      ## grass/shrub
#      saveRDS(mods2save_G, 
#         paste0("./models/glm_beta_model_GrassShrub_", s, "_n", n, 
#         #sample_group, 
#         ".RDS"))
#   }
# }


```


 

```{r pdp_glm, warning = FALSE, fig.width = 8, fig.height=8}
## partial dependence plots
#vip::vip(mod_glmFinal, num_features = 15)

#pdp_all_vars(mod_glmFinal, mod_vars = pred_vars, ylab = 'probability',train = df_small)

#caret::varImp(fit)
```


# session info

Hash of current commit (i.e. to ID the version of the code used)

```{r}
system("git rev-parse HEAD", intern=TRUE)
```

Packages etc.

```{r}
sessionInfo()

```