---
title: "Models of vegetation composition based on climate predictors"
subtitle: "Based on code from Martin Holdrege's Sagebrush-Fire paper"
author: "Alice Stears"
date: "`r lubridate::today()`"
params: 
  run: FALSE
  test_run: FALSE
  save_figs: FALSE
  ecoregion: "shrubGrass" #shrubGrass #forest
  response: "TotalHerbaceousCover" #"CAMCover", "TotalHerbaceousCover", "BareGroundCover",                           "BroadleavedTreeCover_prop", "NeedleLeavedTreeCover_prop","C4Cover_prop", "C3Cover_prop", "ForbCover_prop", "ShrubCover"
  hmod: FALSE
  s: "TotalHerbaceousCover"
  #inter: !r c('afgAGB:MAP' = "afgAGB:MAP")
  sample_group: 1
  byRegion: TRUE
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
---
The data consists of vegetation % cover by functional group from across CONUS (from AIM, FIA, LANDFIRE, and RAP), 
as well as climate variables from DayMet, which have been aggregated into mean interannual conditions accross multiple temporal windows. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE,
                      message = FALSE)
```

# Dependencies 

User defined parameters

```{r}
print(params)
# set to true if want to run for a limited number of rows (i.e. for code testing)
test_run <- params$test_run
save_figs <- params$save_figs
hmod <- params$hmod # whether to include human modification in the model
# by changing the sample_group the model can be fit to a completely different set of rows
sample_group <- params$sample_group 
response <- params$response
# _ann defines this model as being built on annual data
s <- params$s # string to paste to file names e.g., that defines the interactions in the model
# such as (summer ppt * MAT) and annuals*temperature interactions
fit_sample <- TRUE # fit model to a sample of the data
n_train <- 5e4 # sample size of the training data
n_test <- 1e6 # sample size of the testing data (if this is too big the decile dotplot code throws memory errors)
byRegion <- params$byRegion

run <- params$run
```


```{r warning=FALSE, message=FALSE}
# set option so resampled dataset created here reproduces earlier runs of this code with dplyr 1.0.10
source("../../../Functions/glmTransformsIterates.R")
source("../../../Functions/transformPreds.R")
source("../../../Functions/StepBeta_mine.R")
#source("src/fig_params.R")
#source("src/modeling_functions.R")
library(glinternet)
library(terra)
library(tidyterra)
library(sf)
library(caret)
library(betareg)
library(tidyverse)
library(GGally) # for ggpairs()
library(pdp) # for partial dependence plots
library(gridExtra)
library(knitr)
library(patchwork) # for figure insets etc. 
library(ggtext)
library(StepBeta)
theme_set(theme_classic())
library(here)
library(rsample)
```

# read in data

Data compiled in the `prepDataForModels.R` script

```{r}
here::i_am("Analysis/VegComposition/ModelFitting/01_PredictorVarSelection.Rmd")
modDat <- readRDS( here("Data_processed", "CoverData", "DataForModels_spatiallyAveraged_withSoils_noSf.rds"))
## there are some values of the annual wet degree days 5th percentile that have -Inf?? change to lowest value for now? 
modDat[is.infinite(modDat$annWetDegDays_5percentile_3yrAnom), "annWetDegDays_5percentile_3yrAnom"] <- -47.8
## same, but for annual water deficit 95th percentile 
modDat[is.infinite(modDat$annWaterDeficit_95percentile_3yrAnom), "annWaterDeficit_95percentile_3yrAnom"] <- -600

# # Convert total cover variables into proportions (for later use in beta regression models) ... proportions are already scaled from zero to 1
# modDat <- modDat %>%
#   mutate(TotalTreeCover = TotalTreeCover/100,
#          CAMCover = CAMCover/100,
#          TotalHerbaceousCover = TotalHerbaceousCover/100,
#          BareGroundCover = BareGroundCover/100,
#          ShrubCover = ShrubCover/100
#          )
# For all response variables, make sure there are no 0s add or subtract .0001 from each, since the Gamma model framework can't handle that
modDat[modDat$TotalTreeCover == 0 & !is.na(modDat$TotalTreeCover), "TotalTreeCover"] <- 0.0001
modDat[modDat$CAMCover == 0 & !is.na(modDat$CAMCover), "CAMCover"] <- 0.0001
modDat[modDat$TotalHerbaceousCover == 0  & !is.na(modDat$TotalHerbaceousCover), "TotalHerbaceousCover"] <- 0.0001
modDat[modDat$BareGroundCover == 0 & !is.na(modDat$BareGroundCover), "BareGroundCover"] <- 0.0001
modDat[modDat$ShrubCover == 0 & !is.na(modDat$ShrubCover), "ShrubCover"] <- 0.0001
modDat[modDat$BroadleavedTreeCover_prop == 0 & !is.na(modDat$BroadleavedTreeCover_prop), "BroadleavedTreeCover_prop"] <- 0.0001
modDat[modDat$NeedleLeavedTreeCover_prop == 0 & !is.na(modDat$NeedleLeavedTreeCover_prop), "NeedleLeavedTreeCover_prop"] <- 0.0001
modDat[modDat$C4Cover_prop == 0 & !is.na(modDat$C4Cover_prop), "C4Cover_prop"] <- 0.0001
modDat[modDat$C3Cover_prop == 0 & !is.na(modDat$C3Cover_prop), "C3Cover_prop"] <- 0.0001
modDat[modDat$ForbCover_prop == 0 & !is.na(modDat$ForbCover_prop), "ForbCover_prop"] <- 0.0001
# 
# modDat[modDat$TotalTreeCover ==1& !is.na(modDat$TotalTreeCover), "TotalTreeCover"] <- 0.999
# modDat[modDat$CAMCover ==1& !is.na(modDat$CAMCover), "CAMCover"] <- 0.999
# modDat[modDat$TotalHerbaceousCover ==1 & !is.na(modDat$TotalHerbaceousCover), "TotalHerbaceousCover"] <- 0.999
# modDat[modDat$BareGroundCover ==1& !is.na(modDat$BareGroundCover), "BareGroundCover"] <- 0.999
# modDat[modDat$ShrubCover ==1& !is.na(modDat$ShrubCover), "ShrubCover"] <- 0.999
# modDat[modDat$BroadleavedTreeCover_prop ==1& !is.na(modDat$BroadleavedTreeCover_prop), "BroadleavedTreeCover_prop"] <- 0.999
# modDat[modDat$NeedleLeavedTreeCover_prop ==1& !is.na(modDat$NeedleLeavedTreeCover_prop), "NeedleLeavedTreeCover_prop"] <- 0.999
# modDat[modDat$C4Cover_prop ==1& !is.na(modDat$C4Cover_prop), "C4Cover_prop"] <- 0.999
# modDat[modDat$C3Cover_prop ==1& !is.na(modDat$C3Cover_prop), "C3Cover_prop"] <- 0.999
# modDat[modDat$ForbCover_prop ==1& !is.na(modDat$ForbCover_prop), "ForbCover_prop"] <- 0.999

```


# Prep data

```{r rename variables}
set.seed(1234)
modDat_1 <- modDat %>% 
  select(-c(prcp_annTotal:annVPD_min)) %>% 
  # mutate(Lon = st_coordinates(.)[,1], 
  #        Lat = st_coordinates(.)[,2])  %>% 
  # st_drop_geometry() %>% 
  # filter(!is.na(newRegion))
  rename("tmin" = tmin_meanAnnAvg_CLIM, 
     "tmax" = tmax_meanAnnAvg_CLIM, #1
     "tmean" = tmean_meanAnnAvg_CLIM, 
     "prcp" = prcp_meanAnnTotal_CLIM, 
     "t_warm" = T_warmestMonth_meanAnnAvg_CLIM,
     "t_cold" = T_coldestMonth_meanAnnAvg_CLIM, 
     "prcp_wet" = precip_wettestMonth_meanAnnAvg_CLIM,
     "prcp_dry" = precip_driestMonth_meanAnnAvg_CLIM, 
     "prcp_seasonality" = precip_Seasonality_meanAnnAvg_CLIM, #2
     "prcpTempCorr" = PrecipTempCorr_meanAnnAvg_CLIM,  #3
     "abvFreezingMonth" = aboveFreezing_month_meanAnnAvg_CLIM, 
     "isothermality" = isothermality_meanAnnAvg_CLIM, #4
     "annWatDef" = annWaterDeficit_meanAnnAvg_CLIM, 
     "annWetDegDays" = annWetDegDays_meanAnnAvg_CLIM,
     "VPD_mean" = annVPD_mean_meanAnnAvg_CLIM, 
     "VPD_max" = annVPD_max_meanAnnAvg_CLIM, #5
     "VPD_min" = annVPD_min_meanAnnAvg_CLIM, #6
     "VPD_max_95" = annVPD_max_95percentile_CLIM, 
     "annWatDef_95" = annWaterDeficit_95percentile_CLIM, 
     "annWetDegDays_5" = annWetDegDays_5percentile_CLIM, 
     "frostFreeDays_5" = durationFrostFreeDays_5percentile_CLIM, 
     "frostFreeDays" = durationFrostFreeDays_meanAnnAvg_CLIM, 
     "soilDepth" = soilDepth, #7
     "clay" = surfaceClay_perc, 
     "sand" = avgSandPerc_acrossDepth, #8
     "coarse" = avgCoarsePerc_acrossDepth, #9
     "carbon" = avgOrganicCarbonPerc_0_3cm, #10
     "AWHC" = totalAvailableWaterHoldingCapacity,
     ## anomaly variables
     tmean_anom = tmean_meanAnnAvg_3yrAnom, #15
     tmin_anom = tmin_meanAnnAvg_3yrAnom, #16
     tmax_anom = tmax_meanAnnAvg_3yrAnom, #17
    prcp_anom = prcp_meanAnnTotal_3yrAnom, #18
      t_warm_anom = T_warmestMonth_meanAnnAvg_3yrAnom,  #19
     t_cold_anom = T_coldestMonth_meanAnnAvg_3yrAnom, #20
      prcp_wet_anom = precip_wettestMonth_meanAnnAvg_3yrAnom, #21
      precp_dry_anom = precip_driestMonth_meanAnnAvg_3yrAnom,  #22
    prcp_seasonality_anom = precip_Seasonality_meanAnnAvg_3yrAnom, #23 
     prcpTempCorr_anom = PrecipTempCorr_meanAnnAvg_3yrAnom, #24
      aboveFreezingMonth_anom = aboveFreezing_month_meanAnnAvg_3yrAnom, #25  
    isothermality_anom = isothermality_meanAnnAvg_3yrAnom, #26
       annWatDef_anom = annWaterDeficit_meanAnnAvg_3yrAnom, #27
     annWetDegDays_anom = annWetDegDays_meanAnnAvg_3yrAnom,  #28
      VPD_mean_anom = annVPD_mean_meanAnnAvg_3yrAnom, #29
      VPD_min_anom = annVPD_min_meanAnnAvg_3yrAnom,  #30
      VPD_max_anom = annVPD_max_meanAnnAvg_3yrAnom,  #31
     VPD_max_95_anom = annVPD_max_95percentile_3yrAnom, #32
      annWatDef_95_anom = annWaterDeficit_95percentile_3yrAnom, #33 
      annWetDegDays_5_anom = annWetDegDays_5percentile_3yrAnom ,  #34
    frostFreeDays_5_anom = durationFrostFreeDays_5percentile_3yrAnom, #35 
      frostFreeDays_anom = durationFrostFreeDays_meanAnnAvg_3yrAnom #36
  )

# small dataset for if testing the data
if(test_run) {
  modDat_1 <- slice_sample(modDat_1, n = 1e5)
}
```

Identify the ecoregion and response variable type to use in this model run 
```{r get ecoregion variable}
ecoregion <- params$ecoregion
response <- params$response
print(paste0("In this model run, the ecoregion is ", ecoregion," and the response variable is ",response))
```

Subset the data to only include data for the ecoregion of interest
```{r subset data to that ecoregion}

if (ecoregion == "shrubGrass") {
  # select data for the ecoregion of interest
  modDat_1 <- modDat_1 %>%
    filter(newRegion == "dryShrubGrass")
} else if (ecoregion == "forest") {
  # select data for the ecoregion of interest
  modDat_1 <- modDat_1 %>% 
    filter(newRegion %in% c("eastForest", "westForest"))
}

# remove the rows that have no observations for the response variable of interest
modDat_1 <- modDat_1[!is.na(modDat_1[,response]),]
```

## Visualize the response variable 
```{r visualize response variable}
hist(modDat_1[,response], main = paste0("Histogram of ",response),
     xlab = paste0(response))
```


## Visualize the predictor variables

The following are the candidate predictor variables for this ecoregion: 
```{r get candidate predictor variables}
if (ecoregion == "shrubGrass") {
  # select potential predictor variables for the ecoregion of interest
        prednames <-
          c(
"tmean"                  , "prcp"               ,"prcp_seasonality"     ,"prcpTempCorr"        ,   
"isothermality"          , "VPD_max"            ,"sand"                 ,"coarse"              ,   
"AWHC"                   , "tmax_anom"          ,"t_warm_anom"          ,"t_cold_anom"         ,   
"prcp_wet_anom"          , "precp_dry_anom"     ,"prcp_seasonality_anom","prcpTempCorr_anom"   ,   
"aboveFreezingMonth_anom", "isothermality_anom" ,"annWatDef_anom"       ,"annWetDegDays_anom"  ,   
"VPD_min_anom"           , "VPD_max_95_anom"    ,   
"frostFreeDays_5_anom"   )
  
} else if (ecoregion == "forest") {
  # select potential predictor variables for the ecoregion of interest
  prednames <- 
    c(
"tmean"               , "prcp"                , "prcpTempCorr"            ,"isothermality"      , 
"annWatDef"           , "annWetDegDays"       , "sand"                    ,"coarse"             , 
"carbon"              , "AWHC"                , "tmax_anom"               ,"prcp_anom"          , 
"t_warm_anom"         , "t_cold_anom"         , "prcp_wet_anom"           ,"precp_dry_anom"     , 
"prcp_seasonality_anom", "prcpTempCorr_anom"  ,  "aboveFreezingMonth_anom", "isothermality_anom",  
"annWatDef_anom"      , "annWetDegDays_anom"  , "VPD_min_anom"            ,"VPD_max_95_anom"    , 
 "frostFreeDays_5_anom"   
    )
}

# subset the data to only include these predictors, and remove any remaining NAs 
modDat_1 <- modDat_1 %>% 
  select(prednames, response, newRegion, Year.x, Long.x, Lat.x, NA_L1NAME, NA_L2NAME) %>% 
  drop_na()

names(prednames) <- prednames
df_pred <- modDat_1[, prednames]
# 
# # print the list of predictor variables
# knitr::kable(format = "html", data.frame("Possible_Predictors" = prednames)
# ) %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 
```


```{r summary_table}
create_summary <- function(df) {
  df %>% 
    pivot_longer(cols = everything(),
                 names_to = 'variable') %>% 
    group_by(variable) %>% 
    summarise(across(value, .fns = list(mean = ~mean(.x, na.rm = TRUE), min = ~min(.x, na.rm = TRUE), 
                                        median = ~median(.x, na.rm = TRUE), max = ~max(.x, na.rm = TRUE)))) %>% 
    mutate(across(where(is.numeric), round, 4))
}

modDat_1[prednames] %>% 
  create_summary() %>% 
  knitr::kable(caption = 'summaries of possible predictor variables') %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 


# response_summary <- modDat_1 %>% 
#     dplyr::select(#where(is.numeric), -all_of(pred_vars),
#       matches(response)) %>% 
#     create_summary()
# 
# 
# kable(response_summary, 
#       caption = 'summaries of response variables, calculated using paint') %>%
# kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 

```

## Plot predictor vars against each other

```{r pred_v_pred, fig.width=10}
set.seed(12011993)
# function for colors
my_fn <- function(data, mapping, method="p", use="pairwise", ...){
  
  # grab data
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  
  # calculate correlation
  corr <- cor(x, y, method=method, use=use)
  
  # calculate colour based on correlation value
  # Here I have set a correlation of minus one to blue, 
  # zero to white, and one to red 
  # Change this to suit: possibly extend to add as an argument of `my_fn`
  colFn <- colorRampPalette(c("red", "white", "blue"), interpolate ='spline')
  fill <- colFn(100)[findInterval(corr, seq(-1, 1, length=100))]
  
  ggally_cor(data = data, mapping = mapping, size = 2.5, stars = FALSE, 
             digits = 2, colour = I("black"),...) + 
    theme_void() +
    theme(panel.background = element_rect(fill=fill))
  
}
(corrPlot <- modDat_1 %>% 
  select(prednames) %>% 
  slice_sample(n = 5e4) %>% 
  #select(-matches("_")) %>% 
ggpairs( upper = list(continuous = my_fn, size = .1), lower = list(continuous = GGally::wrap("points", alpha = 0.1, size=0.1)), progress = FALSE))
```

## boxplots-- # predictor variables compared to binned response variables

```{r climVar_boxplots, fig.height=9, fig.width=8}
set.seed(12011993)
# vector of name of response variables
vars_response <- response

# longformat dataframes for making boxplots
df_sample_plots <-  modDat_1  %>% 
  slice_sample(n = 5e4) %>% 
   rename(response = all_of(response)) %>% 
  mutate(response = case_when(
    response <= .25 ~ ".25", 
    response > .25 & response <=.5 ~ ".5", 
    response > .5 & response <=.75 ~ ".75", 
    response >= .75  ~ "1", 
  )) %>% 
  select(c(response, prednames)) %>% 
  tidyr::pivot_longer(cols = unname(prednames), 
               names_to = "predictor", 
               values_to = "value"
               )  
 

  ggplot(df_sample_plots, aes_string(x= "response", y = 'value')) +
  geom_boxplot() +
  facet_wrap(~predictor , scales = 'free_y') + 
  ylab("Predictor Variable Values") + 
    xlab(response)

```


# Model Fitting

Repeat the following process x # of times for cross validation [
1. Subset the data into training and test datasets (based on EPA Level II ecoregion--use leave-one-out CV)

2. Then, use cross-validation (based on EPA Level II ecoregion--use leave-one-out CV) within the training dataset to identify the appropriate regularization parameter 

3. Then, fit the model with the regularization parameter identified in step 2 to the entire training dataset identified in step 1 for this iteration 
 
## First, Fit a null model w/ no predictors 
With the full dataset (full response variables) -- use a Gamma family glm with a "log" link function (suited for response variables that are bounded by 0 and are right skewed)
```{r fit null model}
# try using bamlss
nullMod_gamma <- glm(data = modDat_1, formula = modDat_1[,response] ~ 1, family = stats::Gamma(link = "log"))
#plot(nullMod_gamma)

summary(nullMod_gamma)
```
Visualize the level 2 ecoregions and how they differ across environmental space
```{r Visualize ecoregion distribution, fig.width = 8, fig.height=6}
# make a table of n for each region
modDat_1 %>% 
  group_by(NA_L2NAME) %>% 
  dplyr::summarize("Number_Of_Observations" = length(NA_L2NAME)) %>% 
  rename("Level_2_Ecoregion" = NA_L2NAME)%>% 
  kable() %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 

## visualize the variation between groups across environmental space
## make data into spatial format
modDat_1_sf <- modDat_1 %>% 
  st_as_sf(coords = c("Long.x", "Lat.x"), crs = st_crs("PROJCRS[\"unnamed\",\n    BASEGEOGCRS[\"unknown\",\n        DATUM[\"unknown\",\n            ELLIPSOID[\"Spheroid\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1,\n                    ID[\"EPSG\",9001]]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]]],\n    CONVERSION[\"Lambert Conic Conformal (2SP)\",\n        METHOD[\"Lambert Conic Conformal (2SP)\",\n            ID[\"EPSG\",9802]],\n        PARAMETER[\"Latitude of false origin\",42.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-100,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",25,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",60,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"northing\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]"))

us_states <- tigris::states()

cropped_states <- us_states %>%
  dplyr::filter(STUSPS!="HI") %>%
  dplyr::filter(STUSPS!="AK") %>%
  dplyr::filter(STUSPS!="Puerto Rico") %>%
  dplyr::filter(STUSPS!="American Samoa") %>%
  dplyr::filter(STUSPS!="Guam") %>%
  dplyr::filter(STUSPS!="Commonwealth of the Northern Mariana Islands") %>%
  dplyr::filter(STUSPS!="United States Virgin Islands") %>%

  sf::st_sf() %>%
  sf::st_transform(sf::st_crs(modDat_1_sf)) %>%
  sf::st_crop(sf::st_bbox(modDat_1_sf)+c(-1,-1,1,1))

map1 <- ggplot() +
  geom_sf(data=cropped_states,fill='white') +
  geom_sf(data=modDat_1_sf,aes(fill=as.factor(NA_L2NAME)),linewidth=0.5,alpha=0.5) +
  geom_point(data=modDat_1,alpha=0.5, 
             aes(x = Long.x, y = Lat.x, color=as.factor(modDat_1$NA_L2NAME))) +
  #scale_fill_okabeito() +
  #scale_color_okabeito() +
 # theme_default() +
  theme(legend.position = 'none') +
  labs(title = "Level 2 Ecoregions as spatial blocks")

hull <- modDat_1_sf %>%
  ungroup() %>%
  group_by(NA_L2NAME) %>%
  slice(chull(tmean, prcp))

plot1<-ggplot(data=modDat_1_sf,aes(x=tmean,y=prcp)) +
  geom_polygon(data = hull, alpha = 0.25,aes(fill=NA_L2NAME) )+
  geom_point(aes(group=NA_L2NAME,color=NA_L2NAME),alpha=0.25) +
  theme_minimal() + xlab("Annual Average T_mean - long-term average") +
  ylab("Annual Average Precip - long-term average") #+
  #scale_color_okabeito() +
  #scale_fill_okabeito()

plot2<-ggplot(data=modDat_1_sf %>%
                pivot_longer(cols=tmean:prcp),
              aes(x=value,group=name)) +
  # geom_polygon(data = hull, alpha = 0.25,aes(fill=fold) )+
  geom_density(aes(group=NA_L2NAME,fill=NA_L2NAME),alpha=0.25) +
  theme_minimal() +
  facet_wrap(~name,scales='free')# +
  #scale_color_okabeito() +
  #scale_fill_okabeito()
 
library(patchwork)
(combo <- (map1+plot1)/plot2) 
```

Fit a global model with all of the data 
-- within the training set, use level 2 ecoregion for cross-validation to tune lambda in the LASSO model 
```{r fit global model, echo = FALSE}
  # Fit model 
  # penalty values
  lambdas <- 10^seq(0 ,-7, length.out = 200)  # sequence of penalties to test
 
  
  # make model matrix (which includes transformations and interactions)
  ## only include interactions within and between climate and anomaly variables, and within and between soils variables

 ## break predictors into climate, weather, and soils 
  prednames_clim <- prednames[which(prednames %in% c("tmin" , "tmax",  "tmean",
     "prcp", "t_warm" , "t_cold" , "prcp_wet" , "prcp_dry" , "prcp_seasonality", "prcpTempCorr", "abvFreezingMonth" , "isothermality" , "annWatDef" , "annWetDegDays",  "VPD_mean",  "VPD_max",  "VPD_min" , "VPD_max_95", "annWatDef_95", "annWetDegDays_5",  "frostFreeDays_5", "frostFreeDays"))]
  prednames_weath <- prednames[which(prednames %in% c("tmean_anom", "tmin_anom",  "tmax_anom", 
    "prcp_anom",      "t_warm_anom" , "t_cold_anom",  "prcp_wet_anom" , "precp_dry_anom" ,  "prcp_seasonality_anom" ,  "prcpTempCorr_anom" , "aboveFreezingMonth_anom",  "isothermality_anom" ,  "annWatDef_anom", "annWetDegDays_anom",  "VPD_mean_anom" , "VPD_min_anom" , "VPD_max_anom", "VPD_max_95_anom", "annWatDef_95_anom" , "annWetDegDays_5_anom",  "frostFreeDays_5_anom" , "frostFreeDays_anom"))]
  prednames_soils <- prednames[which(prednames %in% c( "soilDepth" , "clay", 
     "sand" , "coarse" ,  "carbon", "AWHC"))]
  
  # get a text string of possible interactions between climate and weather variables
  combos <- data.frame(gtools::permutations(n = length(c(prednames_clim, prednames_weath)),
                                            r = 2,
                                            v = c(prednames_clim, prednames_weath)))
  
  combos$interactions <- paste0(combos$X1, ":", combos$X2)
  
  
  # get a text string of possible interactions between soils variables
  combos_2 <- data.frame(gtools::permutations(n = length(c(prednames_soils)),
                                            r = 2,
                                            v = c(prednames_soils)))
  
  combos_2$interactions <- paste0(combos_2$X1, ":", combos_2$X2)
  
  # make text string of all possible interactions
  int_string <-str_flatten(c(combos$interactions, combos_2$interactions), collapse = " + ")
  
  # get a text string of the possible untransformed model terms
  mod_string <- paste(response, "~", paste(prednames, collapse = "+"))
  
  # make a text string of the model formula w/ interactions included
  modWithInteractions <- paste(mod_string, "+",int_string) 
  # now, make a model formula object
  f <- as.formula(modWithInteractions)
  
  # transform dataframe to model matrix
  X <- model.matrix(f, modDat_1)
  # get response variable
  y <- as.matrix(modDat_1[,response])
  
  # split into training and test sets
  # hold out several years ()
  test_eco <- unique(modDat_1$NA_L2NAME)[7]
  
  # identify the rowID of observations to be in the training and test datasets
  ## randomly select three years to hold out from the training dataset (2017, 2021, 2001)
  #randYears <- sample(unique(modDat_1$Year.x), size = 3)
  
  #train <- which(!(modDat_1$Year.x %in% c(2017, 2021, 2001)))
  #test <- which(modDat_1$Year.x %in% c(2017, 2021, 2001))
  
  #trainDat_all <- modDat_1[train,]
  #testDat_all <- modDat_1[test,]
  
  # get the model matrices for input and response variables
  #X_train <- as.matrix(X[train,])
  #X_test <- as.matrix(X[test,])
  
  #y_train <- modDat_1[train,response]
  #y_test <- modDat_1[test,response]
  
  # get the ecoregions for training lambda
  train_eco <- modDat_1$NA_L2NAME#[train]
  
  # Fit model -----------------------------------------------
  
  # penalty values
  lambdas <- 10^seq(0 ,-7, length.out = 200)  # sequence of penalties to test
  
  # # don't penalize previous year's abundance or meadow ID
  # pen_facts <- rep(1, times = ncol(X))
  # logNtId <- which(colnames(X) == "logNt")
  # meadaId <- grep("meada",colnames(X))
  # pen_facts[c(logNtId,meadaId)] <- 0  
  
  # specify leave-one-year-out cross-validation
  my_folds <- as.numeric(as.factor(train_eco))
 
  if (run == TRUE) {
    fit <- cv.glmnet(
    x = X[,2:ncol(X)], 
    y = y, 
    family = stats::Gamma(link = "log"),
    alpha = 1,  # 0 == ridge regression, 1 == lasso, 0.5 ~~ elastic net
    lambda = lambdas, 
    type.measure="mse",
    #penalty.factor = pen_facts,
    foldid = my_folds,
    standardize = TRUE ## scales variables prior to the model sequence... coefficients are always returned on the original scale
    )
    base::saveRDS(fit, paste0("../ModelFitting/models/", response, "_globalLASSOmod_",ecoregion, ".rds"))
  
  } else {
    fit <- readRDS(paste0("../ModelFitting/models/", response, "_globalLASSOmod_",ecoregion, ".rds"))
  }
  
  
  # save the minimum lambda
  best_lambda <- fit$lambda.min
  print(fit)
  # look at CV score vs penalty plot
  plot(log(fit$lambda),fit$cvm,main= paste0("comparing cross-validation score against log(lambda) \n best log(lambda) = ", round(log(best_lambda),3)))
  
  ## get the coefficients
  coef(fit, s = "lambda.min")
   
  ## predict on the test data
  # lasso model predictions with the optimal lambda
  optimal_pred <- predict(fit,newx=X,s="lambda.min", type = "response")
  
  # # re-fit the model with the specified lambda
  #  fit_bestLambda <- glmnet(
  #   x = X_train, 
  #   y = y_train, 
  #   family = stats::Gamma(link = "log"),
  #   alpha = 1,  # 0 == ridge regression, 1 == lasso, 0.5 ~~ elastic net
  #   lambda = best_lambda, 
  #   type.measure="mse",
  #   #penalty.factor = pen_facts,
  #   #foldid = my_folds,
  #   standardize = FALSE
  # )
  # 
  # print(fit_bestLambda)
  # 
  # ## get the coefficients
  # coef(fit_bestLambda)
  # ## use a subset of held-out data to predict with the fitted (and null) models
  # 
  # ## predict on the test data
  # # lasso model predictions with the optimal lambda
  # optimal_pred_bestLambda <- predict(fit_bestLambda,newx=X_test,s="lambda.min", type = "response")
  # 

  # null model and predictions -- just with plain predictors, no interactions
  #X_null_train <- data.frame(X_train[,c(1:10,13)])
  #X_null_test <- data.frame(X_test[,c(1:10,13)])
  null_fit <- glm(data = as.data.frame(X[,prednames]), formula = y ~ 1, family = stats::Gamma(link = "log"))
  null_pred <- predict(null_fit, newdata = as.data.frame(X), type = "response"
                       )

  # save data
  fullModOut <- list(
    "modelObject" = fit,
    "nullModelObject" = null_fit,
    "modelPredictions" = data.frame(ecoRegion_holdout = rep(test_eco,length(y)),obs=y,
                    pred_opt=optimal_pred[,1], pred_null=null_pred#,
                    #pred_nopenalty=nopen_pred
                    ))
  
  
# calculate correlations between null and optimal model 
my_cors <- c(cor(optimal_pred[,1], y),
            cor(null_pred, y)
            )

# calculate mse between null and optimal model 
my_mse <- c(mean((obs_pred$pred_opt -  y)^2) ,
            mean((obs_pred$pred_null - y)^2)#,
            #mean((obs_pred$pred_nopenalty - obs_pred$obs)^2)
            )

# X_train_glm <-X_train %>% 
#   as.data.frame %>% 
#   cbind(response = y_train)
# # try fitting a glm with the model parameters selected above, just to see what happens
# fit2 <- glm(response ~ prcp:aboveFreezingMonth_anom+ coarse:AWHC + sand:AWHC +  isothermality:coarse +  prcp:frostFreeDays_5_anom + sand:frostFreeDays_5_anom + prcp:isothermality+ isothermality:sand  + tmean:isothermality+prcp:prcp_seasonality + prcp:prcpTempCorr+ prcp:prcpTempCorr_anom+ prcp:t_cold_anom+ prcp:t_warm_anom  + tmean:prcp +   prcp:VPD_max + tmean:sand,
# data = X_train_glm ,
#     family = stats::Gamma(link = "log")
#             )
# get the predictions
# pred_glm <- predict.glm(object = fit2, newdata = as.data.frame(X_test), type = "response")

plot(y = fullModOut$modelPredictions$obs, x = X[,2], xlab = c("tmean"), ylab = (paste(response))) # observed values
points(y = fullModOut$modelPredictions$pred_opt, x = X[,2], col = "red") ## predictions w/ the CV model
points(y = fullModOut$modelPredictions$pred_null, x = X[,2], col = "blue") ## predictions with the null model
#points(y = pred_glm, x = X_test[,2], col = "green") ## predictions with the GLM 
#points(y = optimal_pred_bestLambda, x = X_test[,2], col = "orange") ## predictions with the best-lambda glmnet model
```

The internal cross-validation process to fit the global LASSO model identified an optimal lambda value (regularization parameter) of `r{best_lambda}`. The following coefficients were kept in the model: 
```{r coefficients in the global model}
# the coefficient matrix from the 'best model' -- find and print those coefficients that aren't 0 in a table
mat <- as.matrix(coef(fit)) 
mat2 <- mat[mat[,1] != 0,]
# the coefficient matrix from the 'best lambda model' -- find and print those coefficients that aren't 0 in a table
#mat3 <- as.matrix(coef(fit_bestLambda)) 
#mat4 <- mat[mat[,1] > 0,]

globModTerms <- data.frame("Coefficient_Name" = names(mat2), 
                   "Value_From_LASSO_with_internal_CV" = unname(mat2)#, 
                   #"Coefficient_Name" = names(fit2$coefficients),
                   #"Value_From_glm" = unname(fit2$coefficients),
                   #"Coefficient_Name"= names(mat4),
                   #"Value_From_LASSO_withBestLambda" = unname(mat4)
                   )

kable(globModTerms, col.names = c("Coefficient Name", "Value from LASSO")
      ) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 
```

```{r eval = FALSE, cross-validation }
## code from Tredennick et al. 2020
# try each separate ecoregion as a test set
# make a list to hold output data 
outList <- vector(mode = "list", length = length(sort(unique(modDat_1$NA_L2NAME))))
# obs_pred <- data.frame(ecoregion = character(),obs = numeric(),
#                        pred_opt = numeric(), pred_null = numeric()#,
#                        #pred_nopenalty = numeric()
#                        )
# code to make model matrix  is above(which includes interactions)

  # transform dataframe to model matrix
  X <- model.matrix(f, modDat_1)
  # get response variable
  y <- as.matrix(modDat_1[,response])
  

  # now, loop through so with each iteration, a different ecoregion is held out
 for(i_eco in sort(unique(modDat_1$NA_L2NAME))){
  
  # split into training and test sets
  test_eco <- i_eco
  
  # identify the rowID of observations to be in the training and test datasets
  train <- which(modDat_1$NA_L2NAME!=test_eco)
  test <- which(modDat_1$NA_L2NAME==test_eco)
  
  trainDat_all <- modDat_1[train,]
  testDat_all <- modDat_1[test,]
  
  # get the model matrices for input and response variables
  X_train <- as.matrix(X[train,])
  X_test <- as.matrix(X[test,])
  
  y_train <- modDat_1[train,response]
  y_test <- modDat_1[test,response]
  
  train_eco <- modDat_1$NA_L2NAME[train]
  
  # Fit model -----------------------------------------------
  
  # penalty values
  lambdas <- 10^seq(0 ,-7, length.out = 200)  # sequence of penalties to test
  
  # # don't penalize previous year's abundance or meadow ID
  # pen_facts <- rep(1, times = ncol(X))
  # logNtId <- which(colnames(X) == "logNt")
  # meadaId <- grep("meada",colnames(X))
  # pen_facts[c(logNtId,meadaId)] <- 0  
  
  # specify leave-one-year-out cross-validation
  my_folds <- as.numeric(as.factor(train_eco))
  
  fit <- cv.glmnet(
    x = X_train, 
    y = y_train, 
    #family = gaussian,
    family = stats::Gamma(link = "log"),
    alpha = 1,  # 0 == ridge regression, 1 == lasso, 0.5 ~~ elastic net
    lambda = lambdas, 
    type.measure="mse",
    #penalty.factor = pen_facts,
    foldid = my_folds,
    standardize = TRUE
  )
  
  # save the minimum lambda
  best_lambda <- fit$lambda.min
  # plot(fit)
  coef(fit, s = best_lambda)
  # look at CV score vs penalty plot
  #plot(log(fit$lambda),fit2$cvm,main=test_eco)
  
  # lasso model predictions with the optimal lambda
  optimal_pred <- predict(fit,newx=X_test,s="lambda.min", scale = "response")
  
  # null model and predictions
  null_fit <- glm(data = as.data.frame(X_train), formula = y_train ~ 1, family = stats::Gamma(link = "log"))
  null_pred <- predict(null_fit, newdata = as.data.frame(X_test), scale = "response")

  # save data
  tmp <- data.frame(ecoRegion_holdout = rep(test_eco,length(y_test)),obs=y_test,
                    pred_opt=optimal_pred[,1], pred_null=null_pred#,
                    #pred_nopenalty=nopen_pred
                    )
  # put output into a list 
  tmpList <- list("modelObject" = fit, 
       "modelPredictions" = tmp)
  
  # save model outputs
  outList[[which(sort(unique(modDat_1$NA_L2NAME)) == i_eco)]] <- tmpList
  # save one example of model fits
  # if(i_eco==sort(unique(modDat_1$NA_L2NAME))[1]){
  #   # save model object
  #   saveRDS(fit,file="./ModelResults/glmnet_fit.RDS")
  #   
  #   #saveRDS(nopen,file="./ModelResults/nopenalty_fit.RDS")
  # }
 }
  
 #  # since for the previous run, I predicted on the linear scale rather than the log scale, transform the predictions (just this once, I fixed the code for subsequent runs to predict on the scale of the response)
 # outList <- lapply(outList, function(x) {
 #  tmp <- x$modelPredictions %>% 
 #      mutate(pred_opt = exp(pred_opt),
 #             pred_null = exp(pred_null))
 #  return(list(
 #    "modelObject" = x$modelObject,
 #    "modelPredictions" = tmp
 #  ))
 #      
 #  })
  

# calculate correlations between null and optimal model 
my_cors <- c(cor(obs_pred$pred_opt,obs_pred$obs),
            cor(obs_pred$pred_null,obs_pred$obs)#, 
            #cor(obs_pred$pred_nopenalty,obs_pred$obs)
            )

# calculate mse between null and optimal model 
my_mse <- c(mean((obs_pred$pred_opt - obs_pred$obs)^2) ,
            mean((obs_pred$pred_null - obs_pred$obs)^2)#,
            #mean((obs_pred$pred_nopenalty - obs_pred$obs)^2)
            )

# print results
names(my_cors) <- names(my_mse) <- c("optimal","null"#,"no penalty"
                                     )
print(my_cors)
print(my_mse)

# save obs vs pred results
write.csv(obs_pred,"./ModelResults/obs_vs_pred.csv",row.names=F)

```

```{r code from Gregor for cross-validation... not used currently}
# ## code from Gregor
# # - ++ hold-out strategy (outer loop) ----
# h_all = c("environmentalblock")#,'lastyears',)
# # - ++ cross-validation strategy (inner loop) ----
# cv_all = c('environmentalblock')
# 
# start.time <- Sys.time()
# 
# # data to use are modDat_1 
# 
# ## make data into spatial format
# modDat_1_sf <- modDat_1 %>% 
#   st_as_sf(coords = c("Long", "Lat"), crs = st_crs("PROJCRS[\"unnamed\",\n    BASEGEOGCRS[\"unknown\",\n        DATUM[\"unknown\",\n            ELLIPSOID[\"Spheroid\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1,\n                    ID[\"EPSG\",9001]]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]]],\n    CONVERSION[\"Lambert Conic Conformal (2SP)\",\n        METHOD[\"Lambert Conic Conformal (2SP)\",\n            ID[\"EPSG\",9802]],\n        PARAMETER[\"Latitude of false origin\",42.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-100,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",25,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",60,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"northing\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]"))
# # covariate names are stored in prednames
# 
# # response name is stored in response
# 
#   nfolds = 10 ##AES may need to change, not sure how many 
#   nrepeats = 1
#   
#   
#   
#   for(h_i in 1:length(h_all)){
#     
#     print(paste("Running",h_all[h_i],"hold-outs, starting at",hms::round_hms(Sys.time(), secs = 1, digits = NULL)))
#     
#     outer_split <- hold_out_data_rsample(df = modDat_1, holdout_method = h_all[h_i], n_folds = nfolds, n_repeats = nrepeats, lastnyears = 5)
#     
#     # add a repetition column even if there is only 1 repetition
#     if('Fold01' %in% outer_split$id |  'Resample01' %in% outer_split$id | 'Fold1' %in% outer_split$id |  'Resample1' %in% outer_split$id ){
#       outer_split$id2 = outer_split$id
#       outer_split$id = 'Repeat1'
#     }
#     
#     for(j_i in 1:length(cv_all)){
#       
#       print(paste("Running",h_all[h_i],"hold-outs with",cv_all[j_i],"cross-validation, starting at",hms::round_hms(Sys.time(), secs = 1, digits = NULL)))
#       
#       nested_split <- inner_split_cv(x = data_seedingOutcomes, outer_folds = outer_split, cv_method = cv_all[j_i], n_folds = nfolds, n_repeats = nrepeats)
#       
#       # add a repetition column even if there is only 1 repetition
#       if('Fold01' %in% nested_split$inner_resamples[[1]]$id | 'Resample01' %in% nested_split$inner_resamples[[1]]$id | 'Fold1' %in% nested_split$inner_resamples[[1]]$id |  'Resample1' %in% nested_split$inner_resamples[[1]]$id){
#         for(k in 1:length(nested_split$inner_resamples)){
#           nested_split$inner_resamples[[k]]$id2 = nested_split$inner_resamples[[k]]$id
#           nested_split$inner_resamples[[k]]$id = 'Repeat1'
#         }
#       }
#       
#       outputList[[cumulative.index]]<-replicateRuns(nested_splits = nested_split, hmethod=h_all[h_i], cvmethod=cv_all[j_i])
#       
#       # increment index
#       cumulative.index <- cumulative.index + 1
#       
#     }
#     
#     holdOutList[[h_i]] <- nested_split
#   }
# 
#   
#   saveRDS(outputList,here::here("03_outputs","08_nestedCrossValidation",paste0(current.species,"-nestedCv-longtermclimate-",nfolds,"outerFolds-results.RDS")))
#   saveRDS(holdOutList,here::here("03_outputs","06_modelData",paste0(current.species,"-nestedCv-longtermclimate-",nfolds,"outerFolds-data.RDS")))
#   
# end.time <- Sys.time()
# run.time = end.time-start.time
# run.time
```

## observed vs. predicted values

Predicting on the data

```{r predict on data used for fitting}

  # create prediction for each each model
# (i.e. for each fire proporation variable)
predict_by_response <- function(mod, df) {
  df_out <- df
  response_name <- paste0(response, "_pred")
  df_out <- df_out %>% cbind(predict(mod, newx= df_out, s="lambda.min", type = "response"))
   colnames(df_out)[ncol(df_out)] <- response_name
  return(df_out)
}

pred_glm1 <- predict_by_response(fit, X)

# add back in true y values
pred_glm1 <- pred_glm1 %>% 
  cbind( data.frame("y" = y))
# rename the true response column to not be 'y_test' 
colnames(pred_glm1)[which(colnames(pred_glm1) == "y")] <- paste(response)

# add back in lat/long data 
pred_glm1 <- pred_glm1 %>% 
  cbind(modDat_1[,c("Long.x", "Lat.x", "Year.x")])
```

### Maps of Residuals
For CONUS-wide model
```{r Residual maps, fig.height = 7, fig.width = 8, }

pred_glm1 <- pred_glm1 %>% 
  mutate(resid = .[[response]] - .[[paste0(response,"_pred")]]) 

# rasterize
# get reference raster
test_rast <-  rast("../../../Data_raw/dayMet/rawMonthlyData/orders/70e0da02b9d2d6e8faa8c97d211f3546/Daymet_Monthly_V4R1/data/daymet_v4_prcp_monttl_na_1980.tif") %>% 
  terra::aggregate(fact = 32, fun = "mean")

# rasterize data
plotResid_rast <- pred_glm1 %>% 
         drop_na(resid) %>% 
  #slice_sample(n = 5e4) %>%
  terra::vect(geom = c("Long.x", "Lat.x")) %>% 
  terra::set.crs(crs(test_rast)) %>% 
  terra::rasterize(y = test_rast, 
                   field = "resid", 
                   fun = mean) %>% 
  #terra::aggregate(fact = 2, fun = mean, na.rm = TRUE) %>% 
  terra::crop(ext(-1950000, 1000000, -1800000, 1000000))

# plot
ggplot() + 
  geom_sf(data=cropped_states %>% st_transform(crs = st_crs(test_rast)),fill='white') +
  geom_spatraster(data = plotResid_rast) + 
  ggtitle(paste0("Resids. (response - pred.) from Grass/shrub ecoregion-wide model of ",s),
          subtitle = paste0(paste0(globModTerms$Coefficient_Name[1:5], collapse = " + "), " + \n", paste0(globModTerms$Coefficient_Name[6:length(globModTerms$Coefficient_Name)], collapse = " + "), collapse = " + ")) +
  scale_fill_gradient2(low = "red",
                       mid = "white" ,
                       high = "blue" , 
                       midpoint = 0, 
                       na.value = "grey80")
#   scale_fill_viridis_c(option = "turbo", limits = c(-92, 75), 
#                        na.value = "white",
#                        breaks = c(-92, -40, 0, 40, 75), 
#                        labels = c(-92, -40, 0, 40, 75),
#                        values = c(0,1)
#                        )
# scale_x_continuous(breaks = c(0, .25, .5, .75, 1), 
#                        labels = c("shrub/grass", 0.25, 0.50, 0.75, "forest")) 

# terra::plot(plotResid_rast, main = paste0("Resids. from Best CONUS wide model of ",s), clip = TRUE, 
#             plg = list(title = "resid."), 
#             col = map.pal("curvature"))
```

### Deciles

Binning predictor variables into deciles (actually percentiles) and looking at the mean
predicted probability for each percentile. The use of the word deciles
is just a legacy thing (they started out being actual deciles)

Then predicting on an identical dataset but with warming

```{r}
var_prop_pred <- paste0(response, "_pred")
response_vars <- c(response, var_prop_pred)

prednames_fig <- paste(str_split(globModTerms$Coefficient_Name, ":", simplify = TRUE)) 
prednames_fig <- prednames_fig[prednames_fig>0]
pred_glm1_deciles <- predvars2deciles(pred_glm1,
                                      response_vars = response_vars,
                                        pred_vars = pred_vars)

```



Publication quality quantile plot

```{r decile_plot, fig.height= 10}

# publication quality version
g3 <- decile_dotplot_pq(pred_glm1_deciles, response = response) + ggtitle("Decile Plot")

if(!hmod) {
# obs/pred inset
g4 <- add_dotplot_inset(g3, pred_glm1_deciles)
} else {
  g4 <- g3
}

  
if(save_figs) {
  png(paste0("figures/quantile_plots/quantile_plot_v5_CONUS_wideModel_", s,  ".png"), 
     units = "in", res = 600, width = 5.5, height = 3.5 )
    print(g2)
  dev.off()
}
if (byRegion == TRUE) {

  # publication quality version
g <- decile_dotplot_pq(pred_glm1_deciles_ALL, response = response) + ggtitle("Decile Plot for ecoregion-level model")

if(!hmod) {
# obs/pred inset
g2 <- add_dotplot_inset(g, pred_glm1_deciles_ALL)
} else {
  g2 <- g
}

if(save_figs) {
  png(paste0("figures/quantile_plots/quantile_plot_v5_regionLevelModel", s,  ".png"), 
     units = "in", res = 600, width = 5.5, height = 3.5 )
    print(g2)
  dev.off()
}


g4/g2
}

```

### Deciles Filtered 

20th and 80th percentiles for each climate variable

```{r}
df <- pred_glm1[, pred_vars] #%>% 
  #mutate(MAT = MAT - 273.15) # k to c
map(df, quantile, probs = c(0.2, 0.8), na.rm = TRUE)
```


Filtered 'Decile' plots of data. These plots show each vegetation variable,
but only based on data that falls into the upper and lower two deciles of
each climate variable. 


```{r glm_deciles_filtered, fig.height = 10, fig.width = 5, message = FALSE}
clim_vars <- c("swe_meanAnnAvg_30yr", "tmean_meanAnnAvg_30yr", "prcp_meanAnnTotal_30yr", "precip_Seasonality_meanAnnAvg_30yr", "isothermality_meanAnnAvg_30yr", "annWetDegDays_meanAnnAvg_30yr")
pred_glm1_deciles_filt <- predvars2deciles( pred_glm1, 
                         response_vars = response_vars,
                         pred_vars = pred_vars,
                         filter_var = TRUE,
                         filter_vars = pred_vars) 

decile_dotplot_filtered_pq(pred_glm1_deciles_filt, xvars = clim_vars)
#decile_dotplot_filtered_pq(pred_glm1_deciles_filt)

```



Filtered quantile figure with middle 2 deciles also shown
(this is very memory intensive so no running at the moment)

```{r fig.height = 8, fig.width = 5, eval = TRUE}
pred_glm1_deciles_filt_mid <- predvars2deciles(pred_glm1, 
                         response_vars = response_vars,
                         pred_vars = pred_vars,
                         filter_vars = pred_vars,
                         filter_var = TRUE,
                         add_mid = TRUE)

g <- decile_dotplot_filtered_pq(df = pred_glm1_deciles_filt_mid, xvars = pred_vars)
g

if(save_figs) {x
jpeg(paste0("figures/quantile_plots/quantile_plot_filtered_mid_v1", s, ".jpeg"),
     units = "in", res = 600, width = 5.5, height = 6 )
  g 
dev.off()
}
```



# Save output

```{r save_output}
# glm models
mods2save <- butcher::butcher(mod_glmFinal) # removes some model components so the saved object isn't huge

mods2save$formula <- best_form
#mods2save$pred_vars_inter <- pred_vars_inter # so have interactions
n <- nrow(df_sample)
mods2save$data_rows <- n

if (byRegion == TRUE){
  mods2save_WF <- butcher::butcher(mod_glmFinal_WF) # removes some model components so the saved object isn't huge

mods2save_WF$formula <- best_form_WF
#mods2save$pred_vars_inter <- pred_vars_inter # so have interactions
mods2save_WF$data_rows <- nrow(wf_sample)

mods2save_EF <- butcher::butcher(mod_glmFinal_EF) # removes some model components so the saved object isn't huge

mods2save_EF$formula <- best_form_EF
#mods2save$pred_vars_inter <- pred_vars_inter # so have interactions
mods2save_EF$data_rows <- nrow(ef_sample)

  mods2save_G <- butcher::butcher(mod_glmFinal_G) # removes some model components so the saved object isn't huge

mods2save_G$formula <- best_form_G
#mods2save$pred_vars_inter <- pred_vars_inter # so have interactions
mods2save_G$data_rows <- nrow(g_sample)
}

if(!test_run) {
  saveRDS(mods2save, 
        paste0("./models/glm_beta_model_CONUSwide_", s, "_n", n, 
        #sample_group, 
        ".RDS"))
  if (byRegion == TRUE) {
    ## western forests
     saveRDS(mods2save_WF, 
        paste0("./models/glm_beta_model_WesternForests_", s, "_n", n, 
        #sample_group, 
        ".RDS"))
    ## eastern forests
     saveRDS(mods2save_EF, 
        paste0("./models/glm_beta_model_EasternForests_", s, "_n", n, 
        #sample_group, 
        ".RDS"))
     ## grass/shrub
     saveRDS(mods2save_G, 
        paste0("./models/glm_beta_model_GrassShrub_", s, "_n", n, 
        #sample_group, 
        ".RDS"))
  }
}


```


## partial dependence plots

```{r pdp_glm, warning = FALSE, fig.width = 8, fig.height=8}
#vip::vip(mod_glmFinal, num_features = 15)

#pdp_all_vars(mod_glmFinal, mod_vars = pred_vars, ylab = 'probability',train = df_small)

#caret::varImp(fit)
```


# session info

Hash of current commit (i.e. to ID the version of the code used)

```{r}
system("git rev-parse HEAD", intern=TRUE)
```

Packages etc.

```{r}
sessionInfo()

```